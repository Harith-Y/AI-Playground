# Auto-generated by AI-Playground
# Generated: 2025-12-30T23:48:19.393221
# FastAPI Prediction Service

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import numpy as np
import pandas as pd
import pickle
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')


# ============================================================================
# FASTAPI SETUP
# ============================================================================

app = FastAPI(
    title="Churn Prediction Microservice - Prediction API",
    description="Auto-generated prediction API",
    version="1.0.0"
)

# Load model at startup
MODEL_PATH = 'models/churn_model.pkl'
model = None

@app.on_event("startup")
async def load_model_startup():
    """Load model when API starts."""
    global model
    print(f"Loading model from {MODEL_PATH}...")
    with open(MODEL_PATH, 'rb') as f:
        model = pickle.load(f)
    print("Model loaded successfully!")


# Request/Response models
class PredictionRequest(BaseModel):
    """Request model for predictions."""
    features: List[List[float]] = Field(..., description="Input features")
    return_probabilities: bool = Field(default=True, description="Return class probabilities")
    
    class Config:
        schema_extra = {
            "example": {
                "features": [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]],
                "return_probabilities": True
            }
        }


class PredictionResponse(BaseModel):
    """Response model for predictions."""
    predictions: List
    n_samples: int
    probabilities: Optional[List[List[float]]] = None
    classes: Optional[List] = None


# ============================================================================
# PREDICTION ENDPOINT
# ============================================================================

@app.post("/predict", response_model=PredictionResponse)
async def predict_endpoint(request: PredictionRequest):
    """
    Make predictions on input data.
    
    Args:
        request: Prediction request with features
    
    Returns:
        Predictions and optionally probabilities
    """
    try:
        # Convert features to numpy array
        X = np.array(request.features)
        
        # Make predictions
        predictions = model.predict(X)
        
        response = {
            'predictions': predictions.tolist(),
            'n_samples': len(X)
        }
        
        # Add probabilities if requested and available
        if request.return_probabilities and hasattr(model, 'predict_proba'):
            probabilities = model.predict_proba(X)
            response['probabilities'] = probabilities.tolist()
            
            if hasattr(model, 'classes_'):
                response['classes'] = model.classes_.tolist()
        
        return response
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")


# ============================================================================
# HEALTH CHECK ENDPOINT
# ============================================================================

@app.get("/health")
async def health_check():
    """
    Health check endpoint.
    
    Returns:
        Status of the API
    """
    return {
        'status': 'healthy',
        'model_loaded': model is not None
    }


@app.get("/")
async def root():
    """
    Root endpoint with API information.
    
    Returns:
        API information
    """
    return {
        'message': 'Prediction API',
        'endpoints': {
            '/predict': 'POST - Make predictions',
            '/health': 'GET - Health check',
            '/docs': 'GET - API documentation'
        }
    }
