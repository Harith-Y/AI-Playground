# Customer Churn Prediction - Modular ML Pipeline

Auto-generated by AI-Playground

## Project Structure

ml_pipeline_modules/
  - __init__.py          # Package initialization
  - preprocess.py        # Data preprocessing module
  - train.py            # Model training module
  - evaluate.py         # Model evaluation module
  - main.py             # Pipeline orchestrator
  - README.md           # This file

## Modules

### preprocess.py
Contains the `preprocess_data()` function that applies all preprocessing steps:
- Missing value imputation (mean strategy)
- Standard scaling for numerical features
- OneHot encoding for categorical features

### train.py
Contains training utilities:
- `split_data()` - Split data into train/test sets
- `create_model()` - Create model instance
- `train_model()` - Train the model
- `save_model()` - Save trained model
- `load_model()` - Load saved model
- `predict()` - Make predictions

### evaluate.py
Contains evaluation utilities:
- `evaluate_model()` - Evaluate model performance
- `plot_confusion_matrix()` - Visualize confusion matrix
- `plot_roc_curve()` - Plot ROC curve
- `save_results()` - Save evaluation results to JSON

### main.py
Orchestrates the complete pipeline by importing and using the above modules.

## Usage

### Run Complete Pipeline
python ml_pipeline_modules/main.py train

### Make Predictions on New Data
python ml_pipeline_modules/main.py predict new_data.csv

### Retrain Model
python ml_pipeline_modules/main.py retrain

### Use as Python Package
```python
# Import the package
from ml_pipeline_modules import preprocess_data, train_model, evaluate_model

# Use individual functions
df_clean = preprocess_data(df)
model = train_model(X_train, y_train)
results = evaluate_model(model, X_test, y_test)
```

### Use Individual Modules
```python
# Import preprocessing
from ml_pipeline_modules.preprocess import preprocess_data
df_processed = preprocess_data(df)

# Import training
from ml_pipeline_modules.train import train_model, save_model
model = train_model(X_train, y_train)
save_model(model, 'my_model.pkl')

# Import evaluation
from ml_pipeline_modules.evaluate import evaluate_model, save_results
results = evaluate_model(model, X_test, y_test)
save_results(results, 'results.json')
```

## Requirements

pandas, numpy, scikit-learn, matplotlib, seaborn

Install with: pip install pandas numpy scikit-learn matplotlib seaborn

## Model Details

- **Model Type**: Random Forest Classifier
- **Task**: Classification (Customer Churn)
- **Hyperparameters**:
  - n_estimators: 100
  - max_depth: 10
  - min_samples_split: 5
- **Random State**: 42

## Pipeline Steps

1. **Data Loading**: Load CSV data
2. **Preprocessing**: Imputation, scaling, encoding
3. **Data Splitting**: 80/20 train/test split
4. **Model Training**: Train Random Forest classifier
5. **Model Evaluation**: Compute metrics, generate visualizations
6. **Results Saving**: Save model and evaluation results

## Benefits of Modular Approach

- Each module can be imported independently
- Easy to test individual components
- Reusable across different projects
- Clear separation of concerns
- Production-ready structure
- Can be deployed as a Python package

## Generated Files

All code in this directory was auto-generated by AI-Playground.
You can modify the code as needed for your specific use case.
