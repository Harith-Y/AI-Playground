"""
Example: Modular Code Generation

This script demonstrates how to generate modular Python code that can be
imported and reused across different scripts in a production environment.
"""

from app.ml_engine.code_generation import (
    generate_preprocessing_code,
    generate_training_code
)
import os

print("=" * 80)
print("MODULAR CODE GENERATION EXAMPLE")
print("=" * 80)

# Create output directory
os.makedirs('generated_modules', exist_ok=True)

# ============================================================================
# Step 1: Generate Preprocessing Module
# ============================================================================
print("\n" + "=" * 80)
print("Step 1: Generate Preprocessing Module")
print("=" * 80)

preprocessing_config = {
    'experiment_name': 'Customer Churn Prediction',
    'task_type': 'classification',
    'random_state': 42,
    'dataset_info': {
        'file_path': 'customer_data.csv',
        'file_format': 'csv'
    },
    'preprocessing_steps': [
        {
            'type': 'missing_value_imputation',
            'name': 'Impute Missing Values',
            'parameters': {
                'strategy': 'mean',
                'columns': ['age', 'income', 'tenure']
            }
        },
        {
            'type': 'scaling',
            'name': 'Standard Scaling',
            'parameters': {
                'scaler': 'standard',
                'columns': ['age', 'income', 'tenure']
            }
        },
        {
            'type': 'encoding',
            'name': 'OneHot Encoding',
            'parameters': {
                'encoder': 'onehot',
                'columns': ['gender', 'contract_type']
            }
        }
    ]
}

# Generate preprocessing function (modular)
preprocess_code = generate_preprocessing_code(
    preprocessing_config,
    output_format='function'
)

# Save as module
with open('generated_modules/preprocess.py', 'w') as f:
    f.write('"""Preprocessing Module - Auto-generated"""\n\n')
    f.write('import pandas as pd\nimport numpy as np\n')
    f.write('from sklearn.preprocessing import StandardScaler, OneHotEncoder\n')
    f.write('from sklearn.impute import SimpleImputer\n\n')
    f.write(preprocess_code)

print("[OK] Preprocessing module saved to 'generated_modules/preprocess.py'")

# ============================================================================
# Step 2: Generate Training Module
# ============================================================================
print("\n" + "=" * 80)
print("Step 2: Generate Training Module")
print("=" * 80)

training_config = {
    'experiment_name': 'Customer Churn Prediction',
    'model_type': 'random_forest_classifier',
    'task_type': 'classification',
    'hyperparameters': {
        'n_estimators': 100,
        'max_depth': 10,
        'min_samples_split': 5
    },
    'target_column': 'churn',
    'random_state': 42,
    'test_size': 0.2,
    'model_path': 'models/churn_model.pkl'
}

# Generate training module
train_code = generate_training_code(
    training_config,
    output_format='module'
)

# Save as module
with open('generated_modules/train.py', 'w') as f:
    f.write(train_code)

print("[OK] Training module saved to 'generated_modules/train.py'")

# ============================================================================
# Step 3: Generate Main Pipeline Script
# ============================================================================
print("\n" + "=" * 80)
print("Step 3: Generate Main Pipeline Script")
print("=" * 80)

# Create a main script that imports and uses the modules
main_script = f"""\"\"\"
Main Pipeline Script - Customer Churn Prediction

This script orchestrates the complete ML pipeline by importing
the generated preprocessing and training modules.

Auto-generated by AI-Playground
\"\"\"

import pandas as pd
import numpy as np
from pathlib import Path

# Import generated modules
from preprocess import preprocess_data
from train import split_data, train_model, save_model, load_model, predict

# Configuration
DATA_PATH = 'customer_data.csv'
MODEL_PATH = 'models/churn_model.pkl'
RANDOM_STATE = 42

def main():
    \"\"\"Run the complete ML pipeline.\"\"\"
    print("=" * 80)
    print("Customer Churn Prediction Pipeline")
    print("=" * 80)
    
    # Step 1: Load data
    print("\\n[1/5] Loading data...")
    df = pd.read_csv(DATA_PATH)
    print(f"Loaded {{len(df)}} rows, {{len(df.columns)}} columns")
    
    # Step 2: Preprocess data
    print("\\n[2/5] Preprocessing data...")
    df_processed = preprocess_data(df)
    print(f"Processed shape: {{df_processed.shape}}")
    
    # Step 3: Split data
    print("\\n[3/5] Splitting data...")
    X_train, X_test, y_train, y_test = split_data(df_processed, target_column='churn')
    print(f"Train: {{len(X_train)}} samples, Test: {{len(X_test)}} samples")
    
    # Step 4: Train model
    print("\\n[4/5] Training model...")
    model = train_model(X_train, y_train, verbose=True)
    
    # Step 5: Evaluate and save
    print("\\n[5/5] Evaluating and saving model...")
    test_score = model.score(X_test, y_test)
    print(f"Test score: {{test_score:.4f}}")
    
    # Save model
    save_model(model, MODEL_PATH)
    
    # Make predictions
    predictions = predict(model, X_test)
    print(f"\\nGenerated {{len(predictions)}} predictions")
    
    print("\\n" + "=" * 80)
    print("Pipeline completed successfully!")
    print("=" * 80)


def predict_new_data(data_path: str, model_path: str = MODEL_PATH):
    \"\"\"
    Load a trained model and make predictions on new data.
    
    Args:
        data_path: Path to new data CSV
        model_path: Path to saved model
    
    Returns:
        Predictions array
    \"\"\"
    print(f"Loading model from {{model_path}}...")
    model = load_model(model_path)
    
    print(f"Loading data from {{data_path}}...")
    df = pd.read_csv(data_path)
    
    print("Preprocessing data...")
    df_processed = preprocess_data(df)
    
    print("Making predictions...")
    predictions = predict(model, df_processed)
    
    return predictions


if __name__ == '__main__':
    main()
"""

# Save main script
with open('generated_modules/main.py', 'w', encoding='utf-8') as f:
    f.write(main_script)

print("[OK] Main pipeline script saved to 'generated_modules/main.py'")

# ============================================================================
# Step 4: Generate README
# ============================================================================
print("\n" + "=" * 80)
print("Step 4: Generate README")
print("=" * 80)

readme = """# Customer Churn Prediction - Generated ML Pipeline

Auto-generated by AI-Playground

## Project Structure

generated_modules/
  - preprocess.py      # Data preprocessing module
  - train.py          # Model training module
  - main.py           # Main pipeline orchestrator
  - README.md         # This file

## Modules

### preprocess.py
Contains the `preprocess_data()` function that applies all preprocessing steps:
- Missing value imputation (mean strategy)
- Standard scaling for numerical features
- OneHot encoding for categorical features

### train.py
Contains training utilities:
- `split_data()` - Split data into train/test sets
- `create_model()` - Create model instance
- `train_model()` - Train the model
- `save_model()` - Save trained model
- `load_model()` - Load saved model
- `predict()` - Make predictions

### main.py
Orchestrates the complete pipeline by importing and using the above modules.

## Usage

### Run Complete Pipeline
```python
python main.py
```

### Use Individual Modules
In Python:
  from preprocess import preprocess_data
  df_processed = preprocess_data(df)
  
  from train import train_model, save_model
  model = train_model(X_train, y_train)
  save_model(model, 'my_model.pkl')

### Make Predictions on New Data
  from main import predict_new_data
  predictions = predict_new_data('new_customers.csv')

## Requirements

pandas, numpy, scikit-learn

Install with: pip install pandas numpy scikit-learn

## Model Details

- **Model Type**: Random Forest Classifier
- **Task**: Classification (Customer Churn)
- **Hyperparameters**:
  - n_estimators: 100
  - max_depth: 10
  - min_samples_split: 5
- **Random State**: 42

## Generated Files

All code in this directory was auto-generated by AI-Playground.
You can modify the code as needed for your specific use case.
"""

with open('generated_modules/README.md', 'w', encoding='utf-8') as f:
    f.write(readme)

print("[OK] README saved to 'generated_modules/README.md'")

# ============================================================================
# Summary
# ============================================================================
print("\n" + "=" * 80)
print("MODULAR CODE GENERATION COMPLETE!")
print("=" * 80)
print("\nGenerated modular ML pipeline:")
print("  - preprocess.py: Reusable preprocessing module")
print("  - train.py: Reusable training module")
print("  - main.py: Pipeline orchestrator")
print("  - README.md: Documentation")
print("\nBenefits of modular approach:")
print("  [+] Each module can be imported independently")
print("  [+] Easy to test individual components")
print("  [+] Reusable across different projects")
print("  [+] Clear separation of concerns")
print("  [+] Production-ready structure")
print("\nTo run the pipeline:")
print("  cd generated_modules")
print("  python main.py")
print("=" * 80)
