# Model Comparison Guide

**Feature**: BACKEND-49 - Model Comparison Logic (Multiple Runs)  
**Implementation Date**: December 30, 2025  
**Status**: ‚úÖ Complete

## Overview

The Model Comparison feature allows you to compare multiple model runs side-by-side, automatically rank them based on performance metrics, and receive intelligent recommendations for model selection. This is essential for experiment tracking and choosing the best model for production deployment.

## Key Features

### 1. **Automatic Comparison**

- Compare 2-10 models simultaneously
- Auto-detect task type (classification, regression)
- Auto-select appropriate comparison metrics
- Statistical summaries for each metric

### 2. **Custom Ranking**

- Weighted ranking with custom criteria
- Up to 20 models in ranking
- Composite scoring with transparency
- Normalized metrics for fair comparison

### 3. **Intelligent Recommendations**

- Best model identification
- Performance vs training time trade-offs
- Statistical significance insights
- Ensembling suggestions

## API Endpoints

### Compare Models

**Endpoint**: `POST /api/v1/models/compare`

Compare multiple model runs and get comprehensive analysis.

**Request Body**:

```json
{
  "model_run_ids": [
    "123e4567-e89b-12d3-a456-426614174001",
    "123e4567-e89b-12d3-a456-426614174002",
    "123e4567-e89b-12d3-a456-426614174003"
  ],
  "comparison_metrics": ["accuracy", "f1_score", "precision", "recall"],
  "ranking_criteria": "f1_score",
  "include_statistical_tests": true
}
```

**Parameters**:

- `model_run_ids` (required): List of 2-10 model run UUIDs to compare
- `comparison_metrics` (optional): Specific metrics to compare. If null, auto-detects based on task type
- `ranking_criteria` (optional): Primary metric for ranking. If null, uses best metric for task type (f1_score for classification, r2_score for regression)
- `include_statistical_tests` (optional): Include statistical significance tests (default: false)

**Response**:

```json
{
  "comparison_id": "comp-abc-123",
  "task_type": "classification",
  "total_models": 3,
  "compared_models": [
    {
      "model_run_id": "123e4567-e89b-12d3-a456-426614174001",
      "model_type": "random_forest_classifier",
      "experiment_id": "123e4567-e89b-12d3-a456-426614174000",
      "status": "completed",
      "metrics": {
        "accuracy": 0.95,
        "f1_score": 0.935,
        "precision": 0.94,
        "recall": 0.93
      },
      "hyperparameters": {
        "n_estimators": 100,
        "max_depth": 10
      },
      "training_time": 45.5,
      "created_at": "2025-12-30T10:00:00Z",
      "rank": 1,
      "ranking_score": 0.935
    }
  ],
  "best_model": {
    "model_run_id": "123e4567-e89b-12d3-a456-426614174001",
    "model_type": "random_forest_classifier",
    "rank": 1,
    "ranking_score": 0.935
  },
  "metric_statistics": [
    {
      "metric_name": "accuracy",
      "mean": 0.93,
      "std": 0.02,
      "min": 0.9,
      "max": 0.95,
      "best_model_id": "123e4567-e89b-12d3-a456-426614174001",
      "worst_model_id": "123e4567-e89b-12d3-a456-426614174003"
    }
  ],
  "ranking_criteria": "f1_score",
  "recommendations": [
    "random_forest_classifier achieved the best f1_score of 0.9350",
    "Consider logistic_regression for faster training (5.2s vs 45.5s) with minimal performance drop (0.050)"
  ],
  "timestamp": "2025-12-30T10:00:00Z"
}
```

### Rank Models

**Endpoint**: `POST /api/v1/models/rank`

Rank models using custom weighted criteria for multi-objective optimization.

**Request Body**:

```json
{
  "model_run_ids": [
    "123e4567-e89b-12d3-a456-426614174001",
    "123e4567-e89b-12d3-a456-426614174002"
  ],
  "ranking_weights": {
    "f1_score": 0.5,
    "precision": 0.3,
    "recall": 0.2
  },
  "higher_is_better": {
    "f1_score": true,
    "precision": true,
    "recall": true
  }
}
```

**Parameters**:

- `model_run_ids` (required): List of 2-20 model run UUIDs
- `ranking_weights` (required): Metric weights (must sum to 1.0)
- `higher_is_better` (optional): Dict indicating direction for each metric. Default: true for all

**Response**:

```json
{
  "ranking_id": "rank-xyz-789",
  "ranked_models": [
    {
      "model_run_id": "123e4567-e89b-12d3-a456-426614174001",
      "model_type": "random_forest_classifier",
      "rank": 1,
      "composite_score": 0.935,
      "individual_scores": {
        "f1_score": 0.935,
        "precision": 0.94,
        "recall": 0.93
      },
      "weighted_contributions": {
        "f1_score": 0.4675,
        "precision": 0.282,
        "recall": 0.186
      }
    }
  ],
  "ranking_weights": {
    "f1_score": 0.5,
    "precision": 0.3,
    "recall": 0.2
  },
  "best_model": {
    "model_run_id": "123e4567-e89b-12d3-a456-426614174001",
    "rank": 1,
    "composite_score": 0.935
  },
  "score_range": {
    "min": 0.85,
    "max": 0.935,
    "spread": 0.085
  },
  "timestamp": "2025-12-30T10:00:00Z"
}
```

### List Model Runs

**Endpoint**: `GET /api/v1/models/runs`

List model runs with filtering and pagination.

**Query Parameters**:

- `experiment_id` (optional): Filter by experiment UUID
- `status` (optional): Filter by status (completed, failed, running, pending, cancelled)
- `model_type` (optional): Filter by model type
- `limit` (optional): Maximum results (default: 50, max: 100)
- `offset` (optional): Pagination offset (default: 0)

**Example**:

```bash
GET /api/v1/models/runs?experiment_id=abc-123&status=completed&limit=10
```

## Usage Examples

### Example 1: Compare Classification Models

```python
import httpx

# Compare three classification models
response = httpx.post(
    "http://localhost:8000/api/v1/models/compare",
    json={
        "model_run_ids": [
            "abc-123",  # Random Forest
            "def-456",  # Logistic Regression
            "ghi-789"   # SVM
        ],
        "ranking_criteria": "f1_score"
    }
)

comparison = response.json()

# Get best model
best_model = comparison["best_model"]
print(f"Best model: {best_model['model_type']}")
print(f"F1 Score: {best_model['ranking_score']}")

# Review recommendations
for rec in comparison["recommendations"]:
    print(f"- {rec}")
```

**Output**:

```
Best model: random_forest_classifier
F1 Score: 0.935
- random_forest_classifier achieved the best f1_score of 0.9350
- Consider logistic_regression for faster training (5.2s vs 45.5s) with minimal performance drop (0.050)
- Compared 3 models. Consider hyperparameter tuning on the top model for further improvements.
```

### Example 2: Compare Regression Models

```python
# Compare regression models
response = httpx.post(
    "http://localhost:8000/api/v1/models/compare",
    json={
        "model_run_ids": [
            "model-1",  # Random Forest Regressor
            "model-2",  # Linear Regression
            "model-3"   # Gradient Boosting
        ],
        "ranking_criteria": "r2_score"
    }
)

comparison = response.json()

# Check metric statistics
for stat in comparison["metric_statistics"]:
    print(f"{stat['metric_name']}:")
    print(f"  Mean: {stat['mean']:.4f} ¬± {stat['std']:.4f}")
    print(f"  Range: [{stat['min']:.4f}, {stat['max']:.4f}]")
```

**Output**:

```
r2_score:
  Mean: 0.8850 ¬± 0.0350
  Range: [0.8500, 0.9200]
rmse:
  Mean: 0.1867 ¬± 0.0350
  Range: [0.1500, 0.2200]
```

### Example 3: Custom Weighted Ranking

```python
# Rank models with custom weights
# Scenario: Prioritize precision over recall for fraud detection
response = httpx.post(
    "http://localhost:8000/api/v1/models/rank",
    json={
        "model_run_ids": ["model-1", "model-2", "model-3"],
        "ranking_weights": {
            "precision": 0.6,  # High priority
            "recall": 0.2,     # Lower priority
            "f1_score": 0.2
        }
    }
)

ranking = response.json()

# Review ranked models
for model in ranking["ranked_models"]:
    print(f"Rank {model['rank']}: {model['model_type']}")
    print(f"  Composite Score: {model['composite_score']:.4f}")
    print(f"  Contributions:")
    for metric, contribution in model["weighted_contributions"].items():
        print(f"    {metric}: {contribution:.4f}")
```

**Output**:

```
Rank 1: random_forest_classifier
  Composite Score: 0.9450
  Contributions:
    precision: 0.5640
    recall: 0.1860
    f1_score: 0.1950
Rank 2: svm_classifier
  Composite Score: 0.9200
```

### Example 4: List and Compare Models

```python
# List completed models from an experiment
runs_response = httpx.get(
    "http://localhost:8000/api/v1/models/runs",
    params={
        "experiment_id": "exp-123",
        "status": "completed",
        "limit": 10
    }
)

runs = runs_response.json()
model_ids = [run["model_run_id"] for run in runs]

# Compare all completed models
comparison_response = httpx.post(
    "http://localhost:8000/api/v1/models/compare",
    json={"model_run_ids": model_ids}
)

comparison = comparison_response.json()
print(f"Compared {comparison['total_models']} models")
print(f"Best: {comparison['best_model']['model_type']}")
```

### Example 5: Multi-Objective Ranking

```python
# Balance accuracy, training time, and model complexity
# Normalize training time as a "metric" (lower is better)

response = httpx.post(
    "http://localhost:8000/api/v1/models/rank",
    json={
        "model_run_ids": ["model-1", "model-2", "model-3"],
        "ranking_weights": {
            "f1_score": 0.5,
            "accuracy": 0.3,
            "precision": 0.2
        },
        "higher_is_better": {
            "f1_score": True,
            "accuracy": True,
            "precision": True
        }
    }
)

# Find sweet spot between performance and speed
ranking = response.json()
best_balanced = ranking["ranked_models"][0]

print(f"Best balanced model: {best_balanced['model_type']}")
print(f"Composite score: {best_balanced['composite_score']:.4f}")
```

## Use Cases

### 1. **Experiment Tracking**

Compare all models from an experiment to identify the best performer:

```python
# Get all models from experiment
runs = httpx.get(
    f"/api/v1/models/runs?experiment_id={exp_id}&status=completed"
).json()

# Compare them
comparison = httpx.post(
    "/api/v1/models/compare",
    json={"model_run_ids": [r["model_run_id"] for r in runs]}
).json()

# Log best model
best = comparison["best_model"]
print(f"Best: {best['model_type']} - {best['ranking_score']}")
```

### 2. **Model Selection for Production**

Find the optimal model balancing accuracy and inference speed:

```python
# Compare models with focus on performance
comparison = httpx.post(
    "/api/v1/models/compare",
    json={
        "model_run_ids": candidate_models,
        "ranking_criteria": "f1_score"
    }
).json()

# Check training time recommendations
for rec in comparison["recommendations"]:
    if "faster" in rec.lower():
        print(f"Speed recommendation: {rec}")

# Select model
best_model = comparison["best_model"]
if best_model["training_time"] > 60:  # More than 1 minute
    # Check second-best for speed
    second_best = comparison["compared_models"][1]
    if second_best["training_time"] < 30:
        print(f"Consider {second_best['model_type']} for production")
```

### 3. **Hyperparameter Tuning Evaluation**

Compare tuned versions of the same model:

```python
# Compare different hyperparameter configurations
tuned_runs = [
    "random_forest_v1",  # Default params
    "random_forest_v2",  # Grid search
    "random_forest_v3",  # Bayesian optimization
]

comparison = httpx.post(
    "/api/v1/models/compare",
    json={"model_run_ids": tuned_runs}
).json()

# Evaluate tuning effectiveness
for model in comparison["compared_models"]:
    print(f"{model['model_run_id']}:")
    print(f"  Score: {model['ranking_score']:.4f}")
    print(f"  Training Time: {model['training_time']:.1f}s")
    print(f"  Hyperparameters: {model['hyperparameters']}")
```

### 4. **A/B Testing Models**

Compare models for deployment decisions:

```python
# Compare model A (current) vs model B (new)
comparison = httpx.post(
    "/api/v1/models/compare",
    json={
        "model_run_ids": [
            "model_a_current",
            "model_b_new"
        ],
        "ranking_criteria": "f1_score"
    }
).json()

# Check if new model is better
best = comparison["best_model"]
score_diff = comparison["compared_models"][0]["ranking_score"] - \
             comparison["compared_models"][1]["ranking_score"]

if best["model_run_id"] == "model_b_new" and score_diff > 0.01:
    print("‚úì Deploy model B - significant improvement")
else:
    print("‚úó Keep model A - insufficient improvement")
```

### 5. **Ensemble Model Selection**

Identify top models for ensembling:

```python
# Compare all candidate models
comparison = httpx.post(
    "/api/v1/models/compare",
    json={"model_run_ids": all_candidates}
).json()

# Select top 3 for ensemble
top_3 = comparison["compared_models"][:3]

print("Selected for ensemble:")
for model in top_3:
    print(f"- {model['model_type']}: {model['ranking_score']:.4f}")

# Check diversity (different model types)
model_types = {m["model_type"] for m in top_3}
if len(model_types) >= 2:
    print("‚úì Good diversity for ensemble")
```

## Best Practices

### 1. **Selecting Ranking Criteria**

**Classification**:

- Binary classification: Use `f1_score` or `roc_auc`
- Multi-class: Use `f1_score` (macro) or `accuracy`
- Imbalanced data: Prioritize `f1_score` or `precision/recall`

**Regression**:

- General: Use `r2_score`
- Error-sensitive: Use `rmse` or `mae`
- Relative errors: Use `mape` (mean absolute percentage error)

### 2. **Weighting Strategies**

**Equal weights** (simple averaging):

```python
{"precision": 0.5, "recall": 0.5}
```

**Prioritize one metric** (80/20 rule):

```python
{"precision": 0.8, "recall": 0.2}  # Prioritize precision
```

**Balanced multi-metric** (40/30/30):

```python
{"f1_score": 0.4, "precision": 0.3, "recall": 0.3}
```

**Custom business weights**:

```python
# False negatives cost 5x more than false positives
{"recall": 0.7, "precision": 0.3}
```

### 3. **Comparison Tips**

**Always compare**:

- ‚úÖ Models from same experiment
- ‚úÖ Models on same dataset
- ‚úÖ Models with same task type
- ‚úÖ Completed models with metrics

**Avoid comparing**:

- ‚ùå Classification vs regression models
- ‚ùå Models on different datasets
- ‚ùå Incomplete or failed models
- ‚ùå Models without metrics

### 4. **Interpreting Results**

**Small differences** (< 1%):

- Models are essentially equivalent
- Consider training time and complexity
- Good candidates for ensembling

**Moderate differences** (1-5%):

- Clear winner but not overwhelming
- Evaluate trade-offs (speed, interpretability)
- A/B test in production

**Large differences** (> 5%):

- Clear winner
- Use best model with confidence
- Investigate why others underperform

### 5. **Statistical Significance**

When comparing models with similar performance:

```python
# Enable statistical tests
comparison = httpx.post(
    "/api/v1/models/compare",
    json={
        "model_run_ids": ["model-1", "model-2"],
        "include_statistical_tests": True
    }
).json()

# Check if difference is significant
# (Future feature - currently returns descriptive statistics)
```

## Performance Considerations

### Response Times

| Operation | Models | Typical Time |
| --------- | ------ | ------------ |
| Compare   | 2-3    | < 100ms      |
| Compare   | 5-7    | < 200ms      |
| Compare   | 8-10   | < 300ms      |
| Rank      | 2-10   | < 150ms      |
| Rank      | 11-20  | < 300ms      |

### Database Queries

The comparison service executes:

1. Single query to fetch all model runs
2. In-memory processing for statistics
3. No additional database calls

Optimize by:

- Ensuring model_runs table is indexed on `id`
- Using appropriate `limit` in list endpoint
- Caching comparison results if needed

## Error Handling

### Common Errors

**400 Bad Request**:

```json
{
  "detail": "At least 2 models required for comparison"
}
```

**Solution**: Provide at least 2 model run IDs

**400 Bad Request**:

```json
{
  "detail": "All models must have status 'completed'. Incomplete models: ['abc-123']"
}
```

**Solution**: Wait for models to complete training

**400 Bad Request**:

```json
{
  "detail": "Ranking weights must sum to 1.0, got 0.8"
}
```

**Solution**: Ensure weights sum to exactly 1.0

**404 Not Found**:

```json
{
  "detail": "Model runs not found: ['xyz-789']"
}
```

**Solution**: Verify model run IDs exist

## API Integration Examples

### Python (httpx)

```python
import httpx

async def compare_models_async(model_ids: list[str]) -> dict:
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8000/api/v1/models/compare",
            json={"model_run_ids": model_ids}
        )
        response.raise_for_status()
        return response.json()

# Usage
comparison = await compare_models_async(["model-1", "model-2"])
```

### JavaScript/TypeScript

```typescript
async function compareModels(modelIds: string[]) {
  const response = await fetch("http://localhost:8000/api/v1/models/compare", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ model_run_ids: modelIds }),
  });

  if (!response.ok) {
    throw new Error(`Comparison failed: ${response.statusText}`);
  }

  return await response.json();
}

// Usage
const comparison = await compareModels(["model-1", "model-2"]);
console.log(`Best model: ${comparison.best_model.model_type}`);
```

### cURL

```bash
# Compare models
curl -X POST "http://localhost:8000/api/v1/models/compare" \
  -H "Content-Type: application/json" \
  -d '{
    "model_run_ids": [
      "123e4567-e89b-12d3-a456-426614174001",
      "123e4567-e89b-12d3-a456-426614174002"
    ],
    "ranking_criteria": "f1_score"
  }'

# Rank models with custom weights
curl -X POST "http://localhost:8000/api/v1/models/rank" \
  -H "Content-Type: application/json" \
  -d '{
    "model_run_ids": [
      "123e4567-e89b-12d3-a456-426614174001",
      "123e4567-e89b-12d3-a456-426614174002"
    ],
    "ranking_weights": {
      "f1_score": 0.5,
      "precision": 0.3,
      "recall": 0.2
    }
  }'

# List model runs
curl "http://localhost:8000/api/v1/models/runs?experiment_id=abc-123&status=completed&limit=10"
```

## Testing

### Run Tests

```bash
# Run all model comparison tests
pytest backend/tests/test_model_comparison.py -v

# Run specific test class
pytest backend/tests/test_model_comparison.py::TestModelComparisonService -v

# Run with coverage
pytest backend/tests/test_model_comparison.py --cov=app.services.model_comparison_service --cov-report=html
```

### Test Coverage

- ‚úÖ Classification model comparison
- ‚úÖ Regression model comparison
- ‚úÖ Metric auto-detection
- ‚úÖ Statistics calculation
- ‚úÖ Recommendation generation
- ‚úÖ Custom weighted ranking
- ‚úÖ Validation (incomplete models, missing metrics)
- ‚úÖ Edge cases (identical scores, extreme values)

## Future Enhancements

Potential improvements for model comparison:

1. **Statistical Testing**: Add t-tests, ANOVA for significance
2. **Visualization**: Generate comparison charts and plots
3. **Export Reports**: PDF/Excel reports with full analysis
4. **Model Versioning**: Track model lineage and evolution
5. **Cost Analysis**: Include training cost in comparisons
6. **Automated Selection**: ML-based model selection recommendations
7. **A/B Testing**: Integrated A/B test management
8. **Ensemble Optimization**: Automatic ensemble weight calculation

---

## Summary

Model comparison feature provides:

‚úÖ **Comprehensive Comparison**: Compare 2-10 models with automatic metric detection  
‚úÖ **Custom Ranking**: Weighted ranking for multi-objective optimization  
‚úÖ **Intelligent Recommendations**: Actionable insights for model selection  
‚úÖ **Statistical Analysis**: Mean, std, min, max for each metric  
‚úÖ **Performance/Speed Trade-offs**: Identify faster models with minimal accuracy loss  
‚úÖ **Production-Ready**: Robust validation and error handling

**Ready for production use!** üöÄ
