# ML Engine - Complete Documentation

**Version:** 1.0.0  
**Last Updated:** January 2, 2026  
**Status:** Production Ready

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Installation](#installation)
4. [Quick Start](#quick-start)
5. [Core Modules](#core-modules)
6. [Advanced Features](#advanced-features)
7. [API Reference](#api-reference)
8. [Examples](#examples)
9. [Best Practices](#best-practices)
10. [Performance](#performance)
11. [Troubleshooting](#troubleshooting)
12. [Contributing](#contributing)

---

## ğŸ¯ Overview

The ML Engine is a comprehensive machine learning framework built for the AI-Playground platform. It provides end-to-end ML capabilities from data preprocessing to model deployment.

### Key Features

âœ… **Complete ML Pipeline**
- Data preprocessing and cleaning
- Feature engineering and selection
- Model training and evaluation
- Hyperparameter tuning
- Code generation and deployment

âœ… **Production-Ready**
- Memory-optimized for large datasets
- Incremental learning support
- Model serialization and versioning
- Comprehensive error handling
- Extensive logging

âœ… **Developer-Friendly**
- Scikit-learn compatible interface
- Intuitive API design
- Comprehensive documentation
- 100+ unit tests
- Type hints throughout

âœ… **Flexible & Extensible**
- Modular architecture
- Easy to customize
- Plugin-based model registry
- Custom transformer support

### Supported Tasks

| Task Type | Models | Metrics |
|-----------|--------|---------|
| **Classification** | 6 models | Accuracy, Precision, Recall, F1, ROC-AUC |
| **Regression** | 4 models | RÂ², RMSE, MAE, MSE |
| **Clustering** | 4 models | Silhouette, Davies-Bouldin, Calinski-Harabasz |

### Module Statistics

- **Total Modules:** 50+
- **Lines of Code:** 15,000+
- **Test Coverage:** 85%+
- **Documentation:** 10,000+ lines


---

## ğŸ—ï¸ Architecture

### Directory Structure

```
ml_engine/
â”œâ”€â”€ preprocessing/           # Data preprocessing (8 modules)
â”‚   â”œâ”€â”€ base.py             # Base transformer class
â”‚   â”œâ”€â”€ cleaner.py          # Outlier detection (IQR, Z-score)
â”‚   â”œâ”€â”€ encoder.py          # Categorical encoding
â”‚   â”œâ”€â”€ imputer.py          # Missing value imputation
â”‚   â”œâ”€â”€ scaler.py           # Feature scaling
â”‚   â”œâ”€â”€ oversampling.py     # SMOTE, ADASYN
â”‚   â”œâ”€â”€ undersampling.py    # Random, Tomek Links
â”‚   â”œâ”€â”€ pipeline.py         # Pipeline orchestration
â”‚   â”œâ”€â”€ serializer.py       # Pipeline serialization
â”‚   â”œâ”€â”€ config.py           # Configuration management
â”‚   â””â”€â”€ column_transformer.py # Column-wise transformations
â”‚
â”œâ”€â”€ feature_selection/      # Feature engineering (5 modules)
â”‚   â”œâ”€â”€ variance_threshold.py    # Variance-based selection
â”‚   â”œâ”€â”€ correlation_selector.py  # Correlation-based selection
â”‚   â”œâ”€â”€ mutual_information_selector.py  # MI-based selection
â”‚   â”œâ”€â”€ rfe_selector.py          # Recursive Feature Elimination
â”‚   â””â”€â”€ univariate_selector.py   # Univariate statistical tests
â”‚
â”œâ”€â”€ models/                 # ML models (6 modules)
â”‚   â”œâ”€â”€ base.py            # Base model wrapper
â”‚   â”œâ”€â”€ classification.py  # Classification models
â”‚   â”œâ”€â”€ regression.py      # Regression models
â”‚   â”œâ”€â”€ clustering.py      # Clustering models
â”‚   â”œâ”€â”€ registry.py        # Model factory
â”‚   â””â”€â”€ validation.py      # Model validation
â”‚
â”œâ”€â”€ training/              # Training utilities (5 modules)
â”‚   â”œâ”€â”€ trainer.py         # Generic trainer
â”‚   â”œâ”€â”€ data_split.py      # Train/test splitting
â”‚   â”œâ”€â”€ cross_validation.py # K-fold CV
â”‚   â””â”€â”€ incremental_trainer.py # Incremental learning
â”‚
â”œâ”€â”€ tuning/                # Hyperparameter optimization (5 modules)
â”‚   â”œâ”€â”€ grid_search.py     # Grid search
â”‚   â”œâ”€â”€ random_search.py   # Random search
â”‚   â”œâ”€â”€ bayesian.py        # Bayesian optimization
â”‚   â”œâ”€â”€ search_spaces.py   # Parameter spaces
â”‚   â””â”€â”€ cross_validation.py # CV for tuning
â”‚
â”œâ”€â”€ evaluation/            # Model evaluation (10 modules)
â”‚   â”œâ”€â”€ metrics.py         # Metric computation
â”‚   â”œâ”€â”€ classification_metrics.py  # Classification metrics
â”‚   â”œâ”€â”€ regression_metrics.py      # Regression metrics
â”‚   â”œâ”€â”€ clustering_metrics.py      # Clustering metrics
â”‚   â”œâ”€â”€ confusion_matrix.py        # Confusion matrix
â”‚   â”œâ”€â”€ roc_curve.py              # ROC curve
â”‚   â”œâ”€â”€ pr_curve.py               # Precision-Recall curve
â”‚   â”œâ”€â”€ feature_importance.py     # Feature importance
â”‚   â”œâ”€â”€ residual_analysis.py      # Residual plots
â”‚   â””â”€â”€ visualizations.py         # Visualization utilities
â”‚
â”œâ”€â”€ code_generation/       # Code export (8 modules)
â”‚   â”œâ”€â”€ generator.py       # Main generator
â”‚   â”œâ”€â”€ template_engine.py # Jinja2 template engine
â”‚   â”œâ”€â”€ templates.py       # Code templates
â”‚   â”œâ”€â”€ preprocessing_generator.py  # Preprocessing code
â”‚   â”œâ”€â”€ training_generator.py       # Training code
â”‚   â”œâ”€â”€ prediction_generator.py     # Prediction code
â”‚   â”œâ”€â”€ evaluation_generator.py     # Evaluation code
â”‚   â””â”€â”€ requirements_generator.py   # Requirements.txt
â”‚
â”œâ”€â”€ inference/             # Model inference (2 modules)
â”‚   â””â”€â”€ optimized_predictor.py  # Optimized prediction
â”‚
â”œâ”€â”€ utils/                 # Utilities (5 modules)
â”‚   â”œâ”€â”€ serialization.py   # Model/pipeline serialization
â”‚   â”œâ”€â”€ column_type_detector.py  # Type detection
â”‚   â”œâ”€â”€ dataset_optimizer.py     # Memory optimization
â”‚   â””â”€â”€ THEORY.md          # Theoretical background
â”‚
â”œâ”€â”€ validation/            # Data validation (2 modules)
â”‚   â”œâ”€â”€ edge_case_validator.py  # Edge case handling
â”‚   â””â”€â”€ edge_case_fixes.py      # Automatic fixes
â”‚
â”œâ”€â”€ eda_statistics.py      # EDA analysis
â”œâ”€â”€ correlation_analysis.py # Correlation analysis
â”œâ”€â”€ class_distribution_analysis.py  # Class balance
â””â”€â”€ model_registry.py      # Model registry
```

### Design Principles

1. **Modularity**: Each component is independent and reusable
2. **Consistency**: All modules follow scikit-learn interface
3. **Extensibility**: Easy to add new models and transformers
4. **Performance**: Optimized for memory and speed
5. **Reliability**: Comprehensive error handling and validation

### Data Flow

```
Raw Data
    â†“
[Preprocessing Pipeline]
    â”œâ”€â”€ Cleaning (outliers, duplicates)
    â”œâ”€â”€ Imputation (missing values)
    â”œâ”€â”€ Encoding (categorical variables)
    â”œâ”€â”€ Scaling (normalization)
    â””â”€â”€ Feature Selection
    â†“
Processed Data
    â†“
[Model Training]
    â”œâ”€â”€ Train/Test Split
    â”œâ”€â”€ Cross-Validation
    â”œâ”€â”€ Hyperparameter Tuning
    â””â”€â”€ Model Fitting
    â†“
Trained Model
    â†“
[Evaluation]
    â”œâ”€â”€ Metrics Computation
    â”œâ”€â”€ Visualization
    â””â”€â”€ Feature Importance
    â†“
[Deployment]
    â”œâ”€â”€ Model Serialization
    â”œâ”€â”€ Code Generation
    â””â”€â”€ API Integration
```


---

## ğŸ“¦ Installation

### Requirements

- Python 3.11+
- scikit-learn 1.8.0+
- pandas 2.3.3+
- numpy 2.4.0+

### Install Dependencies

```bash
cd backend
pip install -r requirements.txt
```

### Verify Installation

```python
from app.ml_engine.models.classification import ClassificationModel
from app.ml_engine.preprocessing.pipeline import Pipeline

print("ML Engine installed successfully!")
```

---

## ğŸš€ Quick Start

### 1. Basic Classification

```python
from app.ml_engine.models.classification import ClassificationModel
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Load data
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train model
model = ClassificationModel(model_type='random_forest_classifier')
model.fit(X_train, y_train)

# Predict
predictions = model.predict(X_test)
accuracy = model.score(X_test, y_test)
print(f"Accuracy: {accuracy:.4f}")
```

### 2. Complete Pipeline

```python
from app.ml_engine.preprocessing.pipeline import Pipeline
from app.ml_engine.preprocessing.imputer import MeanImputer
from app.ml_engine.preprocessing.scaler import StandardScaler
from app.ml_engine.preprocessing.encoder import OneHotEncoder
import pandas as pd

# Create pipeline
pipeline = Pipeline(steps=[
    ('imputer', MeanImputer()),
    ('scaler', StandardScaler()),
    ('encoder', OneHotEncoder())
])

# Fit and transform
df = pd.read_csv('data.csv')
df_transformed = pipeline.fit_transform(df)

# Save pipeline
pipeline.save('pipeline.pkl')

# Load and use
loaded_pipeline = Pipeline.load('pipeline.pkl')
new_data_transformed = loaded_pipeline.transform(new_data)
```

### 3. Hyperparameter Tuning

```python
from app.ml_engine.tuning.grid_search import run_grid_search
from sklearn.ensemble import RandomForestClassifier

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, None],
    'min_samples_split': [2, 5, 10]
}

# Run grid search
result = run_grid_search(
    estimator=RandomForestClassifier(),
    param_grid=param_grid,
    X=X_train,
    y=y_train,
    cv=5,
    scoring='accuracy'
)

print(f"Best parameters: {result.best_params}")
print(f"Best score: {result.best_score:.4f}")
```

### 4. Model Evaluation

```python
from app.ml_engine.evaluation.classification_metrics import ClassificationMetrics

# Evaluate model
metrics = ClassificationMetrics(y_test, predictions)

print(f"Accuracy: {metrics.accuracy():.4f}")
print(f"Precision: {metrics.precision():.4f}")
print(f"Recall: {metrics.recall():.4f}")
print(f"F1 Score: {metrics.f1_score():.4f}")

# Get confusion matrix
cm = metrics.confusion_matrix()
print(cm)

# Get classification report
report = metrics.classification_report()
print(report)
```

### 5. Code Generation

```python
from app.ml_engine.code_generation.generator import generate_training_code

# Generate training code
code = generate_training_code(
    model_type='random_forest_classifier',
    preprocessing_steps=['imputer', 'scaler'],
    hyperparameters={'n_estimators': 100, 'max_depth': 10}
)

# Save to file
with open('train_model.py', 'w') as f:
    f.write(code)
```

