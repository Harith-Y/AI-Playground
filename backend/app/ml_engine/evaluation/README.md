# Evaluation Module

Comprehensive model evaluation metrics and visualizations for machine learning models.

## üìã Overview

The evaluation module provides tools to assess model performance across different ML tasks:

- ‚úÖ **Classification Metrics** - Accuracy, precision, recall, F1, AUC-ROC, confusion matrix
- ‚úÖ **Regression Metrics** - MAE, MSE, RMSE, R¬≤, MAPE, SMAPE, RMSPE, Adjusted R¬≤
- ‚úÖ **Residual Analysis Utilities** - Residual stats, standardized residuals, outlier detection
- ‚úÖ **Actual vs Predicted Aggregation** - Scatter-ready payload with error stats and correlations
- ‚úÖ **Clustering Metrics** - Silhouette, Calinski-Harabasz, Davies-Bouldin, inertia
- ‚úÖ **Feature Importance** - Native importances, permutation importance, optional SHAP
- ‚úÖ **Visualizations** - ROC curves, PR curves, confusion matrices, residual plots

## üéØ Classification Metrics

### Features

#### Basic Metrics

- **Accuracy**: Overall correctness (correct predictions / total predictions)
- **Precision**: Positive predictive value (TP / (TP + FP))
- **Recall**: Sensitivity/True positive rate (TP / (TP + FN))
- **F1 Score**: Harmonic mean of precision and recall

#### Probability-Based Metrics

- **AUC-ROC**: Area under Receiver Operating Characteristic curve
- **AUC-PR**: Area under Precision-Recall curve
- **Log Loss**: Logarithmic loss for probability predictions

#### Advanced Metrics

- **Balanced Accuracy**: Average recall per class (handles imbalanced data)
- **Matthews Correlation Coefficient (MCC)**: Correlation between predictions and truth
- **Cohen's Kappa**: Agreement between predictions and truth, accounting for chance

#### Multi-Class Support

- **Averaging Strategies**: binary, micro, macro, weighted
- **Per-Class Metrics**: Individual metrics for each class
- **Confusion Matrix**: With optional normalization

### Quick Start

```python
from app.ml_engine.evaluation import calculate_classification_metrics

# Binary classification
y_true = [0, 1, 1, 0, 1, 0, 1, 0]
y_pred = [0, 1, 0, 0, 1, 1, 1, 0]
y_proba = [
    [0.9, 0.1], [0.2, 0.8], [0.6, 0.4], [0.8, 0.2],
    [0.1, 0.9], [0.4, 0.6], [0.3, 0.7], [0.7, 0.3]
]

metrics = calculate_classification_metrics(
    y_true=y_true,
    y_pred=y_pred,
    y_proba=y_proba,
    average='binary'
)

print(f"Accuracy: {metrics.accuracy:.4f}")
print(f"Precision: {metrics.precision:.4f}")
print(f"Recall: {metrics.recall:.4f}")
print(f"F1 Score: {metrics.f1_score:.4f}")
print(f"AUC-ROC: {metrics.auc_roc:.4f}")
```

### Usage Examples

#### Example 1: Binary Classification

```python
from app.ml_engine.evaluation import ClassificationMetricsCalculator

# Create calculator
calculator = ClassificationMetricsCalculator(average='binary')

# Calculate metrics
metrics = calculator.calculate_metrics(
    y_true=[0, 1, 1, 0, 1],
    y_pred=[0, 1, 0, 0, 1],
    y_proba=[[0.9, 0.1], [0.2, 0.8], [0.6, 0.4], [0.8, 0.2], [0.1, 0.9]]
)

# Access metrics
print(f"Accuracy: {metrics.accuracy:.4f}")
print(f"AUC-ROC: {metrics.auc_roc:.4f}")
print(f"Confusion Matrix:\n{metrics.confusion_matrix}")
```

#### Example 2: Multi-Class Classification

```python
from app.ml_engine.evaluation import calculate_classification_metrics

y_true = [0, 1, 2, 0, 1, 2, 0, 1, 2]
y_pred = [0, 1, 2, 0, 2, 2, 1, 1, 2]
y_proba = [
    [0.8, 0.1, 0.1], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8],
    [0.7, 0.2, 0.1], [0.2, 0.3, 0.5], [0.1, 0.2, 0.7],
    [0.4, 0.5, 0.1], [0.2, 0.7, 0.1], [0.1, 0.1, 0.8]
]
class_names = ['cat', 'dog', 'bird']

metrics = calculate_classification_metrics(
    y_true=y_true,
    y_pred=y_pred,
    y_proba=y_proba,
    average='weighted',
    class_names=class_names,
    include_per_class=True
)

# Overall metrics
print(f"Overall Accuracy: {metrics.accuracy:.4f}")
print(f"Weighted F1: {metrics.f1_score:.4f}")

# Per-class metrics
for class_name, class_metrics in metrics.per_class_metrics.items():
    print(f"\n{class_name}:")
    print(f"  Precision: {class_metrics['precision']:.4f}")
    print(f"  Recall: {class_metrics['recall']:.4f}")
    print(f"  F1: {class_metrics['f1_score']:.4f}")
    print(f"  Support: {class_metrics['support']}")
```

#### Example 3: Confusion Matrix

```python
from app.ml_engine.evaluation import ClassificationMetricsCalculator

calculator = ClassificationMetricsCalculator()

# Get confusion matrix
cm_result = calculator.calculate_confusion_matrix(
    y_true=[0, 1, 2, 0, 1, 2],
    y_pred=[0, 1, 2, 1, 1, 2],
    normalize='true',  # Normalize over true labels
    class_names=['cat', 'dog', 'bird']
)

print("Confusion Matrix (normalized):")
print(cm_result['matrix'])
print(f"Classes: {cm_result['class_names']}")
```

#### Example 4: Classification Report

```python
from app.ml_engine.evaluation import ClassificationMetricsCalculator

calculator = ClassificationMetricsCalculator()

# Get sklearn-style classification report
report = calculator.get_classification_report(
    y_true=[0, 1, 2, 0, 1, 2],
    y_pred=[0, 1, 2, 1, 1, 2],
    class_names=['cat', 'dog', 'bird'],
    output_dict=True
)

print("Classification Report:")
for class_name, metrics in report.items():
    if class_name not in ['accuracy', 'macro avg', 'weighted avg']:
        print(f"{class_name}: {metrics}")
```

#### Example 5: Handling Imbalanced Data

```python
from app.ml_engine.evaluation import calculate_classification_metrics

# Imbalanced dataset (90% class 0, 10% class 1)
y_true = [0] * 90 + [1] * 10
y_pred = [0] * 85 + [1] * 5 + [0] * 10

metrics = calculate_classification_metrics(
    y_true=y_true,
    y_pred=y_pred,
    average='binary',
    include_advanced=True
)

# Regular accuracy might be misleading
print(f"Accuracy: {metrics.accuracy:.4f}")  # High due to imbalance

# Balanced accuracy is more informative
print(f"Balanced Accuracy: {metrics.balanced_accuracy:.4f}")  # Lower, more realistic

# Per-class metrics show the issue
print(f"Precision: {metrics.precision:.4f}")
print(f"Recall: {metrics.recall:.4f}")
```

### API Reference

#### `ClassificationMetricsCalculator`

Main class for calculating classification metrics.

**Constructor Parameters:**

- `average` (str): Averaging strategy - 'binary', 'micro', 'macro', 'weighted'
- `pos_label` (int/str): Positive class label for binary classification (default: 1)
- `labels` (list): List of class labels (auto-detected if None)
- `zero_division` (str/int): Value to return when division by zero (default: 'warn')

**Methods:**

##### `calculate_metrics()`

Calculate comprehensive classification metrics.

```python
def calculate_metrics(
    y_true: Union[np.ndarray, pd.Series, List],
    y_pred: Union[np.ndarray, pd.Series, List],
    y_proba: Optional[Union[np.ndarray, pd.DataFrame]] = None,
    class_names: Optional[List[str]] = None,
    include_per_class: bool = True,
    include_advanced: bool = True
) -> ClassificationMetrics
```

**Parameters:**

- `y_true`: True labels
- `y_pred`: Predicted labels
- `y_proba`: Predicted probabilities (optional, for AUC metrics)
- `class_names`: Names for each class (for reporting)
- `include_per_class`: Calculate per-class metrics
- `include_advanced`: Calculate advanced metrics (MCC, Kappa, etc.)

**Returns:** `ClassificationMetrics` object

##### `calculate_confusion_matrix()`

Calculate confusion matrix with optional normalization.

```python
def calculate_confusion_matrix(
    y_true: Union[np.ndarray, pd.Series, List],
    y_pred: Union[np.ndarray, pd.Series, List],
    normalize: Optional[Literal['true', 'pred', 'all']] = None,
    class_names: Optional[List[str]] = None
) -> Dict[str, Union[np.ndarray, List[str]]]
```

##### `get_classification_report()`

Generate sklearn classification report.

```python
def get_classification_report(
    y_true: Union[np.ndarray, pd.Series, List],
    y_pred: Union[np.ndarray, pd.Series, List],
    class_names: Optional[List[str]] = None,
    output_dict: bool = True
) -> Union[str, Dict]
```

#### `ClassificationMetrics`

Dataclass containing all calculated metrics.

**Attributes:**

- `accuracy` (float): Overall accuracy
- `precision` (float): Precision score
- `recall` (float): Recall score
- `f1_score` (float): F1 score
- `auc_roc` (float, optional): AUC-ROC score
- `auc_pr` (float, optional): AUC-PR score
- `balanced_accuracy` (float, optional): Balanced accuracy
- `matthews_corrcoef` (float, optional): Matthews correlation coefficient
- `cohen_kappa` (float, optional): Cohen's kappa score
- `log_loss` (float, optional): Logarithmic loss
- `confusion_matrix` (np.ndarray, optional): Confusion matrix
- `per_class_metrics` (dict, optional): Per-class metrics breakdown
- `support` (dict, optional): Number of samples per class
- `n_samples` (int, optional): Total number of samples
- `n_classes` (int, optional): Number of classes

**Methods:**

- `to_dict()`: Convert to dictionary (JSON-serializable)
- `__repr__()`: String representation

#### `calculate_classification_metrics()`

Convenience function for quick metric calculation.

```python
def calculate_classification_metrics(
    y_true: Union[np.ndarray, pd.Series, List],
    y_pred: Union[np.ndarray, pd.Series, List],
    y_proba: Optional[Union[np.ndarray, pd.DataFrame]] = None,
    average: Literal['binary', 'micro', 'macro', 'weighted'] = 'weighted',
    class_names: Optional[List[str]] = None,
    include_per_class: bool = True,
    include_advanced: bool = True
) -> ClassificationMetrics
```

### Averaging Strategies

#### Binary

For binary classification only. Calculates metrics for the positive class.

```python
metrics = calculate_classification_metrics(
    y_true=[0, 1, 1, 0, 1],
    y_pred=[0, 1, 0, 0, 1],
    average='binary'
)
```

#### Micro

Calculate metrics globally by counting total TP, FP, FN.

```python
metrics = calculate_classification_metrics(
    y_true=[0, 1, 2, 0, 1, 2],
    y_pred=[0, 1, 2, 1, 1, 2],
    average='micro'
)
```

#### Macro

Calculate metrics for each class, then take unweighted mean.

```python
metrics = calculate_classification_metrics(
    y_true=[0, 1, 2, 0, 1, 2],
    y_pred=[0, 1, 2, 1, 1, 2],
    average='macro'
)
```

#### Weighted

Calculate metrics for each class, then take weighted mean by support.

```python
metrics = calculate_classification_metrics(
    y_true=[0, 1, 2, 0, 1, 2],
    y_pred=[0, 1, 2, 1, 1, 2],
    average='weighted'  # Recommended for imbalanced data
)
```

### Input Formats

The module accepts multiple input formats:

```python
# NumPy arrays
y_true = np.array([0, 1, 1, 0, 1])
y_pred = np.array([0, 1, 0, 0, 1])

# Pandas Series
y_true = pd.Series([0, 1, 1, 0, 1])
y_pred = pd.Series([0, 1, 0, 0, 1])

# Python lists
y_true = [0, 1, 1, 0, 1]
y_pred = [0, 1, 0, 0, 1]

# Probabilities as 2D array (binary)
y_proba = [[0.9, 0.1], [0.2, 0.8], [0.6, 0.4], [0.8, 0.2], [0.1, 0.9]]

# Probabilities as 1D array (binary, class 1 only)
y_proba = [0.1, 0.8, 0.4, 0.2, 0.9]

# Probabilities as DataFrame (multi-class)
y_proba = pd.DataFrame({
    0: [0.8, 0.1, 0.1],
    1: [0.1, 0.8, 0.1],
    2: [0.1, 0.1, 0.8]
})
```

### Error Handling

The module provides clear error messages for common issues:

```python
# Mismatched lengths
try:
    metrics = calculate_classification_metrics(
        y_true=[0, 1, 1],
        y_pred=[0, 1]  # Different length
    )
except ValueError as e:
    print(f"Error: {e}")  # "y_true and y_pred must have same length"

# Zero division handling
metrics = calculate_classification_metrics(
    y_true=[0, 0, 1, 1],
    y_pred=[0, 0, 0, 0],  # No predictions for class 1
    average='binary'
)
# Returns 0.0 for precision/recall instead of error
```

### Integration with ML Pipeline

```python
from app.ml_engine.evaluation import calculate_classification_metrics
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Train model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Get predictions
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)

# Calculate metrics
metrics = calculate_classification_metrics(
    y_true=y_test,
    y_pred=y_pred,
    y_proba=y_proba,
    average='weighted',
    class_names=['class_0', 'class_1', 'class_2']
)

# Export for API response
metrics_dict = metrics.to_dict()
```

## üìâ Residual Analysis

Features:

- Residual statistics: mean, median, std dev, MAE, MSE, RMSE, MAPE (safe when targets contain zeros)
- Distribution insights: min/max, five quantiles (5th/25th/50th/75th/95th percentiles), skewness, kurtosis
- Standardized residuals with configurable z-score threshold for outlier flags
- Optional Shapiro-Wilk normality test and correlation between |residuals| and predictions to hint at heteroscedasticity

### Quick Start

```python
from app.ml_engine.evaluation import analyze_residuals

actual = [3.0, -0.5, 2.0, 7.0]
predicted = [2.5, 0.0, 2.0, 8.0]

result = analyze_residuals(actual, predicted, zscore_threshold=2.0)

print(result.mae)                # 0.5
print(result.outlier_indices)    # []
plot_payload = result.residual_series  # ready for plotting
```

### Returned Fields

- `residuals`, `predicted`, `actual`
- `standardized_residuals` (optional), `outlier_indices`, `outlier_threshold`
- `quantiles`, `mean_error`, `median_error`, `std_error`, `mae`, `mse`, `rmse`, `mape`
- `skewness`, `kurtosis`, `normality_test` (Shapiro statistic and p-value)
- `correlation_abs_residuals_predicted` to flag potential heteroscedasticity patterns

## üéØ Actual vs Predicted Aggregation

Features:

- Validated scatter payload of `actual`, `predicted`, and `residuals`
- Error stats: MAE, MSE, RMSE, MAPE, mean/median error
- Fit diagnostics: Pearson correlation, optional Spearman rank, simple best-fit line (slope/intercept)
- R¬≤ when computable; guards against degenerate inputs

### Quick Start

```python
from app.ml_engine.evaluation import aggregate_actual_vs_predicted

actual = [3.0, -0.5, 2.0, 7.0]
predicted = [2.5, 0.0, 2.0, 8.0]

result = aggregate_actual_vs_predicted(actual, predicted)
print(result.mae)         # 0.5
print(result.pearson_r)   # correlation
print(result.best_fit)    # {'slope': ..., 'intercept': ...}
scatter = result.series   # ready for plotting
```

## üß© Clustering Metrics

Features:

- Silhouette score (supports sampling and configurable distance metric)
- Calinski-Harabasz and Davies-Bouldin indices
- Inertia (uses model.inertia\_ when available; otherwise computed manually)
- Cluster size summary with noise point count for algorithms like DBSCAN

### Quick Start

```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from app.ml_engine.evaluation import calculate_clustering_metrics

X, _ = make_blobs(n_samples=50, centers=2, cluster_std=0.6, random_state=42)
model = KMeans(n_clusters=2, random_state=42, n_init=10).fit(X)

metrics = calculate_clustering_metrics(X, model.labels_, model=model)
print(metrics.silhouette)
print(metrics.cluster_sizes)  # {'0': 25, '1': 25}
payload = metrics.to_dict()
```

## üîç Feature Importance

Features:

- Native model importances via `feature_importances_` (tree models) and `coef_` (linear models)
- Permutation importance fallback for any estimator with `score`/`predict`
- Optional SHAP-based importances (install `shap` to enable)
- Ranked output helper for API payloads and plots

### Quick Start

```python
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from app.ml_engine.evaluation import calculate_feature_importance, calculate_permutation_importance

X, y = load_iris(return_X_y=True)
model = RandomForestClassifier(random_state=42, n_estimators=50).fit(X, y)

result = calculate_feature_importance(model, X, y)
print(result.method)          # feature_importances_
print(result.importances)     # {'sepal length (cm)': 0.34, ...}
ranked = result.to_ranked_list()
# [{'feature': 'sepal length (cm)', 'importance': 0.34, 'rank': 1}, ...]

# Permutation importance (works for any estimator with score/predict)
perm_result = calculate_permutation_importance(model, X, y, scoring="accuracy", n_repeats=5)
print(perm_result.method)     # permutation
```

## üß™ Testing

Comprehensive test suite with 80+ test cases covering:

- Binary and multi-class classification
- All averaging strategies
- Probability-based metrics
- Per-class metrics
- Edge cases (perfect predictions, all wrong, imbalanced data)
- Input validation and error handling

Run tests:

```bash
pytest backend/tests/ml_engine/evaluation/test_classification_metrics.py -v
pytest backend/tests/ml_engine/evaluation/test_classification_metrics.py --cov=app.ml_engine.evaluation
```

## üìö References

- [scikit-learn Metrics Documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)
- [Confusion Matrix Explained](https://en.wikipedia.org/wiki/Confusion_matrix)
- [ROC and AUC Explained](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)
- [Precision-Recall Curves](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)



## üìù Notes

- All metrics are calculated using scikit-learn's implementations for consistency
- Probability-based metrics (AUC-ROC, AUC-PR, log loss) require `y_proba` parameter
- For imbalanced datasets, use `balanced_accuracy` and `weighted` averaging
- Confusion matrix can be normalized over true labels, predicted labels, or all samples
- Per-class metrics provide detailed breakdown for multi-class problems

---

## üìä Implementation Details

### Module Status

| Component | Status | Tests | Coverage |
|-----------|--------|-------|----------|
| Classification Metrics | ‚úÖ Complete | 29/29 passing | 100% |
| Confusion Matrix | ‚úÖ Complete | Included in Classification Metrics | 100% |
| Residual Analysis | ‚úÖ Complete | Comprehensive | High |
| Actual vs Predicted | ‚úÖ Complete | Comprehensive | High |
| Clustering Metrics | ‚úÖ Complete | Comprehensive | High |
| Feature Importance | ‚úÖ Complete | Comprehensive | High |
| ROC Curve Generation | ‚úÖ Complete | Comprehensive | High |
| PR Curve Generation | ‚úÖ Complete | Comprehensive | High |
| Regression Metrics | ‚úÖ Complete | Comprehensive | High |

### Files Structure

```
backend/app/ml_engine/evaluation/
‚îú‚îÄ‚îÄ classification_metrics.py      # Classification metrics (~650 lines)
‚îú‚îÄ‚îÄ regression_metrics.py          # Regression metrics (~560 lines)
‚îú‚îÄ‚îÄ residual_analysis.py           # Residual analysis
‚îú‚îÄ‚îÄ actual_vs_predicted.py         # Scatter payload aggregation
‚îú‚îÄ‚îÄ clustering_metrics.py          # Clustering evaluation
‚îú‚îÄ‚îÄ feature_importance.py          # Feature importance
‚îú‚îÄ‚îÄ roc_curve.py                   # ROC curve generation (~610 lines)
‚îú‚îÄ‚îÄ pr_curve.py                    # PR curve generation (~650 lines)
‚îú‚îÄ‚îÄ confusion_matrix.py            # Confusion matrix utilities
‚îú‚îÄ‚îÄ visualizations.py              # Visualization helpers
‚îú‚îÄ‚îÄ metrics.py                     # Common metrics utilities
‚îú‚îÄ‚îÄ README.md                      # This file
‚îî‚îÄ‚îÄ __init__.py                    # Module exports

backend/tests/ml_engine/evaluation/
‚îú‚îÄ‚îÄ test_classification_metrics.py  # 29 tests
‚îú‚îÄ‚îÄ test_regression_metrics.py      # Regression tests
‚îú‚îÄ‚îÄ test_residual_analysis.py       # Residual analysis tests
‚îú‚îÄ‚îÄ test_actual_vs_predicted.py     # Scatter plot tests
‚îú‚îÄ‚îÄ test_clustering_metrics.py      # Clustering tests
‚îú‚îÄ‚îÄ test_feature_importance.py      # Feature importance tests
‚îú‚îÄ‚îÄ test_roc_curve.py               # ROC curve tests
‚îî‚îÄ‚îÄ test_pr_curve.py                # PR curve tests

backend/examples/
‚îî‚îÄ‚îÄ classification_metrics_example.py  # Working examples (~150 lines)
```

### Test Coverage Summary

**Classification Metrics** (29 tests, 100% passing):
- ‚úÖ Binary classification (5 tests)
- ‚úÖ Multi-class classification (4 tests)
- ‚úÖ Per-class metrics (2 tests)
- ‚úÖ Advanced metrics (1 test)
- ‚úÖ Confusion matrix (2 tests)
- ‚úÖ Input formats (3 tests)
- ‚úÖ Error handling (4 tests)
- ‚úÖ Edge cases (5 tests)
- ‚úÖ Convenience function (2 tests)
- ‚úÖ Averaging strategies (1 test)

**Run all evaluation tests:**
```bash
# All evaluation module tests
pytest backend/tests/ml_engine/evaluation/ -v

# With coverage report
pytest backend/tests/ml_engine/evaluation/ --cov=app.ml_engine.evaluation --cov-report=html

# Specific component
pytest backend/tests/ml_engine/evaluation/test_classification_metrics.py -v
```

### Performance Characteristics

**Classification Metrics:**
- **Time Complexity**: O(n) for most metrics, O(n log n) for AUC calculations
- **Space Complexity**: O(n) for confusion matrix, O(c¬≤) for multi-class (c = classes)
- **Typical Runtime**: 
  - <10ms for 1,000 samples
  - <100ms for 10,000 samples
  - <1s for 100,000 samples

**Memory Usage:**
- Minimal overhead for basic metrics
- Confusion matrix: ~4 bytes √ó c¬≤ for c classes
- Per-class metrics: ~100 bytes √ó c for c classes

### Integration Examples

#### With FastAPI Endpoints

```python
from fastapi import APIRouter
from app.ml_engine.evaluation import calculate_classification_metrics

router = APIRouter()

@router.post("/api/v1/models/{run_id}/evaluate")
async def evaluate_model(run_id: str, X_test: pd.DataFrame, y_test: pd.Series):
    # Load model and get predictions
    model = load_model(run_id)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
    
    # Calculate metrics
    metrics = calculate_classification_metrics(
        y_true=y_test,
        y_pred=y_pred,
        y_proba=y_proba,
        average='weighted',
        include_per_class=True,
        include_advanced=True
    )
    
    # Return JSON response
    return {
        "model_run_id": run_id,
        "metrics": metrics.to_dict(),
        "timestamp": datetime.now().isoformat()
    }
```

#### With Training Pipeline

```python
from app.ml_engine.evaluation import calculate_classification_metrics
from app.ml_engine.training import train_model
from app.models import ModelRun

def train_and_evaluate(experiment_id: str, dataset_id: str, model_config: dict):
    # Load data
    X_train, X_test, y_train, y_test = load_and_split_data(dataset_id)
    
    # Train model
    model = train_model(X_train, y_train, model_config)
    
    # Get predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    y_train_proba = model.predict_proba(X_train) if hasattr(model, 'predict_proba') else None
    y_test_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
    
    # Calculate metrics for both sets
    train_metrics = calculate_classification_metrics(
        y_true=y_train,
        y_pred=y_train_pred,
        y_proba=y_train_proba,
        average='weighted'
    )
    
    test_metrics = calculate_classification_metrics(
        y_true=y_test,
        y_pred=y_test_pred,
        y_proba=y_test_proba,
        average='weighted',
        include_per_class=True,
        include_advanced=True
    )
    
    # Save to database
    model_run = ModelRun(
        experiment_id=experiment_id,
        model_type=model_config['model_type'],
        metrics={
            'train': train_metrics.to_dict(),
            'test': test_metrics.to_dict()
        },
        status='completed'
    )
    db.session.add(model_run)
    db.session.commit()
    
    return model_run
```

#### With Model Comparison

```python
from app.ml_engine.evaluation import calculate_classification_metrics
import pandas as pd

def compare_models(models: dict, X_test: pd.DataFrame, y_test: pd.Series):
    """Compare multiple models on the same test set."""
    results = []
    
    for model_name, model in models.items():
        # Get predictions
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
        
        # Calculate metrics
        metrics = calculate_classification_metrics(
            y_true=y_test,
            y_pred=y_pred,
            y_proba=y_proba,
            average='weighted'
        )
        
        # Store results
        results.append({
            'model': model_name,
            'accuracy': metrics.accuracy,
            'precision': metrics.precision,
            'recall': metrics.recall,
            'f1_score': metrics.f1_score,
            'auc_roc': metrics.auc_roc
        })
    
    # Create comparison DataFrame
    comparison_df = pd.DataFrame(results)
    comparison_df = comparison_df.sort_values('f1_score', ascending=False)
    
    return comparison_df
```

### Known Issues & Fixes

**Issue 1: Import Errors (Fixed)**
- **Problem**: Multiple files importing `get_logger` from wrong module
- **Solution**: Changed imports from `app.core.logging_config` to `app.utils.logger`
- **Status**: ‚úÖ Resolved

**Issue 2: Repr Format Error (Fixed)**
- **Problem**: `__repr__` method failed when `auc_roc` was None
- **Solution**: Added conditional formatting for optional metrics
- **Status**: ‚úÖ Resolved

**Issue 3: Test Expectation (Fixed)**
- **Problem**: Test expected accuracy of 0.6667 but actual was 0.7778
- **Solution**: Corrected test expectation based on manual verification
- **Status**: ‚úÖ Resolved

### Design Patterns Used

1. **Dataclass Pattern**: `ClassificationMetrics` for clean data storage with automatic `__init__` and `__repr__`
2. **Calculator Pattern**: `ClassificationMetricsCalculator` separates calculation logic from data storage
3. **Convenience Function**: `calculate_classification_metrics()` provides quick access without instantiation
4. **Strategy Pattern**: Multiple averaging strategies (binary, micro, macro, weighted)
5. **Builder Pattern**: Flexible metric calculation with optional components (per-class, advanced metrics)

### Dependencies

```python
# Core dependencies
scikit-learn>=1.3.0  # Metrics implementations
numpy>=1.24.0        # Array operations
pandas>=2.0.0        # DataFrame support

# Optional dependencies
shap>=0.42.0        # SHAP-based feature importance (optional)
matplotlib>=3.7.0    # Visualizations (future)
seaborn>=0.12.0     # Enhanced visualizations (future)
```

### Error Handling Strategy

The module implements comprehensive error handling:

```python
# Input validation
if len(y_true) != len(y_pred):
    raise ValueError("y_true and y_pred must have same length")

# Type checking with clear messages
if not isinstance(y_proba, (np.ndarray, pd.DataFrame, type(None))):
    raise TypeError("y_proba must be numpy array, DataFrame, or None")

# Graceful degradation
try:
    auc_roc = roc_auc_score(y_true, y_proba[:, 1])
except (ValueError, IndexError):
    logger.warning("Could not calculate AUC-ROC, returning None")
    auc_roc = None

# Zero division protection
precision = precision_score(y_true, y_pred, zero_division=0)
```

### Future Enhancements

**Planned Features:**
- [ ] Interactive visualization dashboards (plotly/dash)
- [ ] Enhanced model explainability (LIME integration)
- [ ] Cross-validation metrics aggregation
- [ ] Statistical significance tests for model comparison
- [ ] Custom metric definitions API
- [ ] Metric history tracking and versioning
- [ ] Automated metric alerts/thresholds
- [ ] A/B testing statistical framework
- [ ] Fairness and bias metrics
- [ ] Model drift detection

**API Improvements:**
- [ ] Async metric calculation for large datasets
- [ ] Batch metric computation
- [ ] Streaming metrics for online learning
- [ ] Metric caching for repeated calculations
- [ ] Export to multiple formats (JSON, CSV, HTML)

### Contributing

When adding new metrics:

1. **Add to appropriate module** (`classification_metrics.py`, `regression_metrics.py`, etc.)
2. **Update the dataclass** to include new metric fields
3. **Write comprehensive tests** (unit + integration + edge cases)
4. **Update documentation** with examples and API reference
5. **Add to `__init__.py` exports** for easy importing
6. **Update this README** with usage examples
7. **Consider performance** for large datasets
8. **Ensure JSON serialization** works via `to_dict()`

### Support & Contact

For issues, questions, or contributions:
- Check existing tests for usage examples
- Review API reference documentation
- Run `pytest -v` to verify implementation
- Check logs for detailed error messages
