"""
Code Generation Templates Module

Provides Jinja2 templates for generating production-ready Python code
from ML experiments. Users can export their entire ML pipeline as code.

Templates include:
- Data loading and preprocessing
- Feature engineering
- Model training
- Model evaluation
- Prediction/inference
- Complete notebooks
- FastAPI inference services
- Requirements.txt

Based on: ML-TO-DO.md > ML-62
"""

from typing import Dict, Any, List, Optional
from jinja2 import Template

# ============================================================================
# IMPORTS TEMPLATE
# ============================================================================

IMPORTS_TEMPLATE = """# Auto-generated by AI-Playground
# Generated: {{ timestamp }}
# Experiment: {{ experiment_name }}

import pandas as pd
import numpy as np
{% if task_type in ['classification', 'regression'] %}
from sklearn.model_selection import train_test_split
{% endif %}
{% if preprocessing_steps %}
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer
{% endif %}
{% if model_type %}
{{ model_imports }}
{% endif %}
{% if include_evaluation %}
from sklearn.metrics import (
    {% if task_type == 'classification' %}
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
    {% elif task_type == 'regression' %}
    mean_absolute_error, mean_squared_error, r2_score,
    mean_absolute_percentage_error
    {% elif task_type == 'clustering' %}
    silhouette_score, davies_bouldin_score, calinski_harabasz_score
    {% endif %}
)
{% endif %}
import pickle
import json
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
RANDOM_STATE = {{ random_state }}
np.random.seed(RANDOM_STATE)
"""

# ============================================================================
# DATA LOADING TEMPLATE
# ============================================================================

DATA_LOADING_TEMPLATE = """
# ============================================================================
# DATA LOADING
# ============================================================================

def load_data(file_path: str) -> pd.DataFrame:
    \"\"\"
    Load dataset from file.
    
    Args:
        file_path: Path to the dataset file
    
    Returns:
        Loaded DataFrame
    \"\"\"
    print(f"Loading data from {file_path}...")
    
    {% if file_format == 'csv' %}
    df = pd.read_csv(file_path)
    {% elif file_format == 'excel' %}
    df = pd.read_excel(file_path)
    {% elif file_format == 'json' %}
    df = pd.read_json(file_path)
    {% elif file_format == 'parquet' %}
    df = pd.read_parquet(file_path)
    {% else %}
    df = pd.read_csv(file_path)  # Default to CSV
    {% endif %}
    
    print(f"Loaded {len(df)} rows and {len(df.columns)} columns")
    print(f"Columns: {list(df.columns)}")
    
    return df


# Load the dataset
df = load_data('{{ dataset_path }}')

# Display basic information
print("\\nDataset Info:")
print(df.info())
print("\\nFirst few rows:")
print(df.head())
print("\\nBasic statistics:")
print(df.describe())
"""

# ============================================================================
# PREPROCESSING TEMPLATE
# ============================================================================

PREPROCESSING_TEMPLATE = """
# ============================================================================
# DATA PREPROCESSING
# ============================================================================

def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:
    \"\"\"
    Apply preprocessing steps to the dataset.
    
    Args:
        df: Input DataFrame
    
    Returns:
        Preprocessed DataFrame
    \"\"\"
    df = df.copy()
    print("\\nApplying preprocessing steps...")
    
    {% for step in preprocessing_steps %}
    # Step {{ loop.index }}: {{ step.name }}
    print(f"  - {{ step.description }}")
    {% if step.type == 'missing_value_imputation' %}
    # Handle missing values in {{ step.columns|join(', ') }}
    from sklearn.impute import SimpleImputer
    imputer = SimpleImputer(strategy='{{ step.strategy }}')
    df[{{ step.columns }}] = imputer.fit_transform(df[{{ step.columns }}])
    
    {% elif step.type == 'outlier_detection' %}
    # Detect and handle outliers using {{ step.method }}
    {% if step.method == 'iqr' %}
    for col in {{ step.columns }}:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - {{ step.threshold }} * IQR
        upper_bound = Q3 + {{ step.threshold }} * IQR
        {% if step.action == 'clip' %}
        df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)
        {% elif step.action == 'remove' %}
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
        {% endif %}
    {% elif step.method == 'zscore' %}
    for col in {{ step.columns }}:
        mean = df[col].mean()
        std = df[col].std()
        lower_bound = mean - {{ step.threshold }} * std
        upper_bound = mean + {{ step.threshold }} * std
        {% if step.action == 'clip' %}
        df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)
        {% elif step.action == 'remove' %}
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
        {% endif %}
    {% endif %}
    
    {% elif step.type == 'scaling' %}
    # Scale features: {{ step.columns|join(', ') }}
    {% if step.scaler == 'standard' %}
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    {% elif step.scaler == 'minmax' %}
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    {% elif step.scaler == 'robust' %}
    from sklearn.preprocessing import RobustScaler
    scaler = RobustScaler()
    {% endif %}
    df[{{ step.columns }}] = scaler.fit_transform(df[{{ step.columns }}])
    
    {% elif step.type == 'encoding' %}
    # Encode categorical features: {{ step.columns|join(', ') }}
    {% if step.encoder == 'onehot' %}
    from sklearn.preprocessing import OneHotEncoder
    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    encoded = encoder.fit_transform(df[{{ step.columns }}])
    encoded_df = pd.DataFrame(
        encoded,
        columns=encoder.get_feature_names_out({{ step.columns }}),
        index=df.index
    )
    df = df.drop(columns={{ step.columns }})
    df = pd.concat([df, encoded_df], axis=1)
    {% elif step.encoder == 'label' %}
    from sklearn.preprocessing import LabelEncoder
    for col in {{ step.columns }}:
        encoder = LabelEncoder()
        df[col] = encoder.fit_transform(df[col].astype(str))
    {% endif %}
    
    {% elif step.type == 'feature_selection' %}
    # Feature selection: {{ step.method }}
    {% if step.method == 'variance_threshold' %}
    from sklearn.feature_selection import VarianceThreshold
    selector = VarianceThreshold(threshold={{ step.threshold }})
    selected_features = df[{{ step.columns }}].columns[selector.fit(df[{{ step.columns }}]).get_support()]
    df = df[list(selected_features) + [col for col in df.columns if col not in {{ step.columns }}]]
    {% elif step.method == 'correlation' %}
    # Keep features with correlation > {{ step.threshold }} with target
    correlations = df[{{ step.columns }}].corrwith(df['{{ step.target }}']).abs()
    selected_features = correlations[correlations > {{ step.threshold }}].index.tolist()
    df = df[selected_features + ['{{ step.target }}']]
    {% endif %}
    {% endif %}
    
    {% endfor %}
    
    print(f"Preprocessing complete. Shape: {df.shape}")
    return df


# Apply preprocessing
df = preprocess_data(df)
"""

# ============================================================================
# FEATURE ENGINEERING TEMPLATE
# ============================================================================

FEATURE_ENGINEERING_TEMPLATE = """
# ============================================================================
# FEATURE ENGINEERING
# ============================================================================

def engineer_features(df: pd.DataFrame) -> pd.DataFrame:
    \"\"\"
    Create new features from existing ones.
    
    Args:
        df: Input DataFrame
    
    Returns:
        DataFrame with engineered features
    \"\"\"
    df = df.copy()
    print("\\nEngineering features...")
    
    {% for feature in engineered_features %}
    # {{ feature.description }}
    {% if feature.type == 'polynomial' %}
    from sklearn.preprocessing import PolynomialFeatures
    poly = PolynomialFeatures(degree={{ feature.degree }}, include_bias=False)
    poly_features = poly.fit_transform(df[{{ feature.columns }}])
    poly_df = pd.DataFrame(
        poly_features,
        columns=poly.get_feature_names_out({{ feature.columns }}),
        index=df.index
    )
    df = pd.concat([df, poly_df], axis=1)
    
    {% elif feature.type == 'interaction' %}
    # Create interaction features
    {% for col1, col2 in feature.interactions %}
    df['{{ col1 }}_x_{{ col2 }}'] = df['{{ col1 }}'] * df['{{ col2 }}']
    {% endfor %}
    
    {% elif feature.type == 'binning' %}
    # Bin continuous features
    df['{{ feature.new_column }}'] = pd.cut(
        df['{{ feature.column }}'],
        bins={{ feature.bins }},
        labels={{ feature.labels }}
    )
    
    {% elif feature.type == 'aggregation' %}
    # Aggregate features
    df['{{ feature.new_column }}'] = df[{{ feature.columns }}].{{ feature.operation }}(axis=1)
    {% endif %}
    
    {% endfor %}
    
    print(f"Feature engineering complete. Shape: {df.shape}")
    return df


{% if engineered_features %}
# Apply feature engineering
df = engineer_features(df)
{% endif %}
"""

# ============================================================================
# TRAIN-TEST SPLIT TEMPLATE
# ============================================================================

TRAIN_TEST_SPLIT_TEMPLATE = """
# ============================================================================
# TRAIN-TEST SPLIT
# ============================================================================

# Separate features and target
X = df.drop(columns=['{{ target_column }}'])
y = df['{{ target_column }}']

print(f"\\nFeatures shape: {X.shape}")
print(f"Target shape: {y.shape}")
print(f"Feature columns: {list(X.columns)}")

# Split data into train, validation, and test sets
{% if validation_split %}
# First split: train + validation vs test
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y,
    test_size={{ test_size }},
    random_state=RANDOM_STATE
    {% if stratify %}, stratify=y{% endif %}
)

# Second split: train vs validation
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp,
    test_size={{ validation_size }},
    random_state=RANDOM_STATE
    {% if stratify %}, stratify=y_temp{% endif %}
)

print(f"\\nTrain set: {len(X_train)} samples")
print(f"Validation set: {len(X_val)} samples")
print(f"Test set: {len(X_test)} samples")
{% else %}
# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size={{ test_size }},
    random_state=RANDOM_STATE
    {% if stratify %}, stratify=y{% endif %}
)

print(f"\\nTrain set: {len(X_train)} samples")
print(f"Test set: {len(X_test)} samples")
{% endif %}
"""

# ============================================================================
# MODEL TRAINING TEMPLATE
# ============================================================================

MODEL_TRAINING_TEMPLATE = """
# ============================================================================
# MODEL TRAINING
# ============================================================================

# Initialize model
print("\\nInitializing {{ model_name }}...")
model = {{ model_class }}(
    {% for param, value in model_params.items() %}
    {{ param }}={{ value }}{{ ',' if not loop.last else '' }}
    {% endfor %}
)

# Train model
print("Training model...")
import time
start_time = time.time()

model.fit(X_train, y_train)

training_time = time.time() - start_time
print(f"Training completed in {training_time:.2f} seconds")

# Display model parameters
print("\\nModel parameters:")
print(model.get_params())
"""

# ============================================================================
# MODEL EVALUATION TEMPLATE
# ============================================================================

MODEL_EVALUATION_TEMPLATE = """
# ============================================================================
# MODEL EVALUATION
# ============================================================================

def evaluate_model(model, X, y, dataset_name="Dataset"):
    \"\"\"
    Evaluate model performance.
    
    Args:
        model: Trained model
        X: Features
        y: True labels/values
        dataset_name: Name of the dataset (for display)
    
    Returns:
        Dictionary of metrics
    \"\"\"
    print(f"\\nEvaluating on {dataset_name}...")
    
    # Make predictions
    y_pred = model.predict(X)
    {% if task_type == 'classification' and has_predict_proba %}
    y_proba = model.predict_proba(X)
    {% endif %}
    
    metrics = {}
    
    {% if task_type == 'classification' %}
    # Classification metrics
    metrics['accuracy'] = accuracy_score(y, y_pred)
    metrics['precision'] = precision_score(y, y_pred, average='{{ average }}', zero_division=0)
    metrics['recall'] = recall_score(y, y_pred, average='{{ average }}', zero_division=0)
    metrics['f1_score'] = f1_score(y, y_pred, average='{{ average }}', zero_division=0)
    
    {% if has_predict_proba %}
    # AUC-ROC (if probabilities available)
    try:
        {% if is_binary %}
        metrics['auc_roc'] = roc_auc_score(y, y_proba[:, 1])
        {% else %}
        metrics['auc_roc'] = roc_auc_score(y, y_proba, multi_class='ovr', average='{{ average }}')
        {% endif %}
    except Exception as e:
        print(f"Could not calculate AUC-ROC: {e}")
    {% endif %}
    
    # Confusion matrix
    cm = confusion_matrix(y, y_pred)
    print("\\nConfusion Matrix:")
    print(cm)
    
    # Classification report
    print("\\nClassification Report:")
    print(classification_report(y, y_pred, zero_division=0))
    
    {% elif task_type == 'regression' %}
    # Regression metrics
    metrics['mae'] = mean_absolute_error(y, y_pred)
    metrics['mse'] = mean_squared_error(y, y_pred)
    metrics['rmse'] = np.sqrt(metrics['mse'])
    metrics['r2'] = r2_score(y, y_pred)
    
    # MAPE (if no zeros in y)
    if not np.any(y == 0):
        metrics['mape'] = mean_absolute_percentage_error(y, y_pred) * 100
    
    {% elif task_type == 'clustering' %}
    # Clustering metrics
    metrics['silhouette'] = silhouette_score(X, y_pred)
    metrics['davies_bouldin'] = davies_bouldin_score(X, y_pred)
    metrics['calinski_harabasz'] = calinski_harabasz_score(X, y_pred)
    {% endif %}
    
    # Print metrics
    print(f"\\n{dataset_name} Metrics:")
    for metric_name, metric_value in metrics.items():
        print(f"  {metric_name}: {metric_value:.4f}")
    
    return metrics


# Evaluate on training set
train_metrics = evaluate_model(model, X_train, y_train, "Training Set")

{% if validation_split %}
# Evaluate on validation set
val_metrics = evaluate_model(model, X_val, y_val, "Validation Set")
{% endif %}

# Evaluate on test set
test_metrics = evaluate_model(model, X_test, y_test, "Test Set")

# Save metrics to file
metrics_dict = {
    'train': train_metrics,
    {% if validation_split %}
    'validation': val_metrics,
    {% endif %}
    'test': test_metrics
}

with open('metrics.json', 'w') as f:
    json.dump(metrics_dict, f, indent=2)
print("\\nMetrics saved to metrics.json")
"""

# ============================================================================
# MODEL SAVING TEMPLATE
# ============================================================================

MODEL_SAVING_TEMPLATE = """
# ============================================================================
# SAVE MODEL
# ============================================================================

# Create output directory
output_dir = Path('{{ output_dir }}')
output_dir.mkdir(parents=True, exist_ok=True)

# Save model
model_path = output_dir / 'model.pkl'
with open(model_path, 'wb') as f:
    pickle.dump(model, f)
print(f"\\nModel saved to {model_path}")

# Save feature names
feature_names = list(X_train.columns)
with open(output_dir / 'feature_names.json', 'w') as f:
    json.dump(feature_names, f, indent=2)
print(f"Feature names saved to {output_dir / 'feature_names.json'}")

# Save model metadata
metadata = {
    'model_type': '{{ model_name }}',
    'task_type': '{{ task_type }}',
    'n_features': len(feature_names),
    'feature_names': feature_names,
    'target_column': '{{ target_column }}',
    'random_state': RANDOM_STATE,
    'training_time': training_time,
    'test_metrics': test_metrics
}

with open(output_dir / 'metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)
print(f"Metadata saved to {output_dir / 'metadata.json'}")
"""

# ============================================================================
# PREDICTION TEMPLATE
# ============================================================================

PREDICTION_TEMPLATE = """
# ============================================================================
# PREDICTION FUNCTION
# ============================================================================

def predict_new_data(model, X_new: pd.DataFrame) -> np.ndarray:
    \"\"\"
    Make predictions on new data.
    
    Args:
        model: Trained model
        X_new: New features (must have same columns as training data)
    
    Returns:
        Predictions
    \"\"\"
    # Ensure columns match training data
    expected_columns = {{ feature_names }}
    
    if list(X_new.columns) != expected_columns:
        print("Warning: Column mismatch. Reordering columns...")
        X_new = X_new[expected_columns]
    
    # Make predictions
    predictions = model.predict(X_new)
    
    {% if task_type == 'classification' and has_predict_proba %}
    # Get probabilities
    probabilities = model.predict_proba(X_new)
    
    return predictions, probabilities
    {% else %}
    return predictions
    {% endif %}


# Example usage:
# new_data = pd.DataFrame({...})  # Your new data
# predictions = predict_new_data(model, new_data)
# print(predictions)
"""

# ============================================================================
# MAIN TEMPLATE
# ============================================================================

MAIN_TEMPLATE = """
# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == '__main__':
    print("=" * 80)
    print("ML Pipeline Execution")
    print("=" * 80)
    
    # All steps are executed above
    print("\\n" + "=" * 80)
    print("Pipeline completed successfully!")
    print("=" * 80)
    
    print("\\nGenerated files:")
    print("  - model.pkl: Trained model")
    print("  - feature_names.json: Feature names")
    print("  - metadata.json: Model metadata")
    print("  - metrics.json: Evaluation metrics")
"""

# ============================================================================
# FASTAPI INFERENCE SERVICE TEMPLATE
# ============================================================================

FASTAPI_TEMPLATE = """# Auto-generated FastAPI inference service
# Generated: {{ timestamp }}

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
import pickle
import json
import numpy as np
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any

# Initialize FastAPI app
app = FastAPI(
    title="{{ model_name }} Inference API",
    description="Auto-generated inference service for {{ experiment_name }}",
    version="1.0.0"
)

# Load model and metadata
MODEL_PATH = Path("{{ output_dir }}") / "model.pkl"
METADATA_PATH = Path("{{ output_dir }}") / "metadata.json"

with open(MODEL_PATH, 'rb') as f:
    model = pickle.load(f)

with open(METADATA_PATH, 'r') as f:
    metadata = json.load(f)

feature_names = metadata['feature_names']


# Request/Response models
class PredictionRequest(BaseModel):
    features: Dict[str, Any] = Field(..., description="Feature values")
    
    class Config:
        json_schema_extra = {
            "example": {
                "features": {name: 0.0 for name in feature_names[:5]}
            }
        }


class PredictionResponse(BaseModel):
    prediction: {% if task_type == 'classification' %}int{% else %}float{% endif %}
    {% if task_type == 'classification' and has_predict_proba %}
    probabilities: List[float]
    {% endif %}
    model_type: str
    

@app.get("/")
def root():
    \"\"\"Root endpoint with API information.\"\"\"
    return {
        "message": "{{ model_name }} Inference API",
        "model_type": metadata['model_type'],
        "n_features": metadata['n_features'],
        "endpoints": {
            "/predict": "Make predictions",
            "/health": "Health check",
            "/model-info": "Model information"
        }
    }


@app.get("/health")
def health_check():
    \"\"\"Health check endpoint.\"\"\"
    return {"status": "healthy", "model_loaded": model is not None}


@app.get("/model-info")
def model_info():
    \"\"\"Get model information.\"\"\"
    return metadata


@app.post("/predict", response_model=PredictionResponse)
def predict(request: PredictionRequest):
    \"\"\"
    Make prediction on input features.
    
    Args:
        request: Prediction request with features
    
    Returns:
        Prediction response
    \"\"\"
    try:
        # Convert features to DataFrame
        df = pd.DataFrame([request.features])
        
        # Ensure all required features are present
        missing_features = set(feature_names) - set(df.columns)
        if missing_features:
            raise HTTPException(
                status_code=400,
                detail=f"Missing features: {missing_features}"
            )
        
        # Reorder columns to match training data
        df = df[feature_names]
        
        # Make prediction
        prediction = model.predict(df)[0]
        
        response = {
            "prediction": {% if task_type == 'classification' %}int(prediction){% else %}float(prediction){% endif %},
            "model_type": metadata['model_type']
        }
        
        {% if task_type == 'classification' and has_predict_proba %}
        # Get probabilities
        probabilities = model.predict_proba(df)[0].tolist()
        response["probabilities"] = probabilities
        {% endif %}
        
        return response
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
"""

# ============================================================================
# REQUIREMENTS.TXT TEMPLATE
# ============================================================================

REQUIREMENTS_TEMPLATE = """# Auto-generated requirements.txt
# Generated: {{ timestamp }}

# Core dependencies
pandas=={{ pandas_version }}
numpy=={{ numpy_version }}
scikit-learn=={{ sklearn_version }}

{% if model_type in ['xgboost', 'XGBoost'] %}
# XGBoost
xgboost=={{ xgboost_version }}
{% endif %}

{% if model_type in ['lightgbm', 'LightGBM'] %}
# LightGBM
lightgbm=={{ lightgbm_version }}
{% endif %}

{% if model_type in ['catboost', 'CatBoost'] %}
# CatBoost
catboost=={{ catboost_version }}
{% endif %}

{% if include_visualization %}
# Visualization
matplotlib=={{ matplotlib_version }}
seaborn=={{ seaborn_version }}
plotly=={{ plotly_version }}
{% endif %}

{% if include_fastapi %}
# FastAPI inference service
fastapi=={{ fastapi_version }}
uvicorn=={{ uvicorn_version }}
pydantic=={{ pydantic_version }}
{% endif %}

{% if include_notebook %}
# Jupyter notebook
jupyter=={{ jupyter_version }}
ipykernel=={{ ipykernel_version }}
{% endif %}
"""

# ============================================================================
# JUPYTER NOTEBOOK TEMPLATE (as JSON structure)
# ============================================================================

NOTEBOOK_TEMPLATE = {
    "cells": [],  # Will be populated dynamically
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}


def create_notebook_cell(cell_type: str, source: List[str], metadata: Optional[Dict] = None):
    """
    Create a Jupyter notebook cell.
    
    Args:
        cell_type: 'code' or 'markdown'
        source: List of source lines
        metadata: Optional cell metadata
    
    Returns:
        Cell dictionary
    """
    cell = {
        "cell_type": cell_type,
        "metadata": metadata or {},
        "source": source
    }
    
    if cell_type == "code":
        cell["execution_count"] = None
        cell["outputs"] = []
    
    return cell


# ============================================================================
# TEMPLATE REGISTRY
# ============================================================================

TEMPLATES = {
    'imports': Template(IMPORTS_TEMPLATE),
    'data_loading': Template(DATA_LOADING_TEMPLATE),
    'preprocessing': Template(PREPROCESSING_TEMPLATE),
    'feature_engineering': Template(FEATURE_ENGINEERING_TEMPLATE),
    'train_test_split': Template(TRAIN_TEST_SPLIT_TEMPLATE),
    'model_training': Template(MODEL_TRAINING_TEMPLATE),
    'model_evaluation': Template(MODEL_EVALUATION_TEMPLATE),
    'model_saving': Template(MODEL_SAVING_TEMPLATE),
    'prediction': Template(PREDICTION_TEMPLATE),
    'main': Template(MAIN_TEMPLATE),
    'fastapi': Template(FASTAPI_TEMPLATE),
    'requirements': Template(REQUIREMENTS_TEMPLATE),
}


def get_template(template_name: str) -> Template:
    """
    Get a template by name.
    
    Args:
        template_name: Name of the template
    
    Returns:
        Jinja2 Template object
    
    Raises:
        KeyError: If template not found
    """
    if template_name not in TEMPLATES:
        raise KeyError(f"Template '{template_name}' not found. Available: {list(TEMPLATES.keys())}")
    
    return TEMPLATES[template_name]
