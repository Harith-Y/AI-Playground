"""
Evaluation Module - Customer Segmentation

Auto-generated by AI-Playground
Generated: 2025-12-30T23:00:47.444821
Task Type: clustering

This module can be imported and used in other scripts:
    from evaluate import evaluate_model, save_results
"""


# Auto-generated by AI-Playground
# Generated: 2025-12-30T23:00:47.444821
# Experiment: Customer Segmentation

import numpy as np
import pandas as pd
import json
from pathlib import Path
from typing import Dict, Any, Optional, Tuple

from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score,
    davies_bouldin_score
)

import warnings
warnings.filterwarnings('ignore')



# Configuration
RANDOM_STATE = 42
RESULTS_PATH = 'evaluation_results.json'


# ============================================================================
# CLUSTERING EVALUATION
# ============================================================================

def evaluate_model(
    model,
    X_test: np.ndarray,
    labels: Optional[np.ndarray] = None
) -> Dict[str, Any]:
    """
    Evaluate clustering model performance.
    
    Args:
        model: Trained clustering model
        X_test: Test features
        labels: Cluster labels (optional, will be computed if not provided)
    
    Returns:
        Dictionary containing evaluation metrics
    """
    print("Evaluating clustering model...")
    print(f"Test samples: {len(X_test)}")
    
    # Get cluster labels if not provided
    if labels is None:
        if hasattr(model, 'labels_'):
            labels = model.labels_
        elif hasattr(model, 'predict'):
            labels = model.predict(X_test)
        else:
            raise ValueError("Could not get cluster labels from model")
    
    results = {
        'task_type': 'clustering',
        'n_samples': len(X_test),
        'n_clusters': len(np.unique(labels[labels != -1])),  # Exclude noise points
        'metrics': {}
    }
    
    # Silhouette Score
    try:
        # Filter out noise points (label -1) for DBSCAN
        mask = labels != -1
        if np.sum(mask) > 1 and len(np.unique(labels[mask])) > 1:
            silhouette = silhouette_score(X_test[mask], labels[mask])
            results['metrics']['silhouette_score'] = float(silhouette)
            print(f"Silhouette Score: {silhouette:.4f}")
        else:
            print("Not enough valid clusters for silhouette score")
    except Exception as e:
        print(f"Could not compute silhouette score: {e}")
    
    # Calinski-Harabasz Score
    try:
        mask = labels != -1
        if np.sum(mask) > 1 and len(np.unique(labels[mask])) > 1:
            ch_score = calinski_harabasz_score(X_test[mask], labels[mask])
            results['metrics']['calinski_harabasz_score'] = float(ch_score)
            print(f"Calinski-Harabasz Score: {ch_score:.4f}")
    except Exception as e:
        print(f"Could not compute Calinski-Harabasz score: {e}")
    
    # Davies-Bouldin Score
    try:
        mask = labels != -1
        if np.sum(mask) > 1 and len(np.unique(labels[mask])) > 1:
            db_score = davies_bouldin_score(X_test[mask], labels[mask])
            results['metrics']['davies_bouldin_score'] = float(db_score)
            print(f"Davies-Bouldin Score: {db_score:.4f}")
    except Exception as e:
        print(f"Could not compute Davies-Bouldin score: {e}")
    
    # Cluster Sizes
    unique, counts = np.unique(labels, return_counts=True)
    cluster_sizes = dict(zip(unique.tolist(), counts.tolist()))
    results['cluster_sizes'] = cluster_sizes
    print(f"\nCluster Sizes: {cluster_sizes}")
    
    print(f"\nEvaluation complete!")
    return results


# ============================================================================
# SAVE RESULTS
# ============================================================================

def save_results(results: Dict[str, Any], filepath: str = 'evaluation_results.json'):
    """
    Save evaluation results to JSON file.
    
    Args:
        results: Evaluation results dictionary
        filepath: Path to save results
    """
    # Ensure directory exists
    Path(filepath).parent.mkdir(parents=True, exist_ok=True)
    
    # Save to JSON
    with open(filepath, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nResults saved to {filepath}")



if __name__ == '__main__':
    """
    Example usage when run as a script.
    """
    print("=" * 80)
    print("Evaluation Module - Customer Segmentation")
    print("=" * 80)
    
    # Example: Load your model and data here
    # model = load_model('model.pkl')
    # X_test, y_test = load_test_data()
    
    # Evaluate model
    # results = evaluate_model(model, X_test, y_test)
    
    # Save results
    # save_results(results, RESULTS_PATH)
    
    print("\nTo use this module in another script:")
    print("  from evaluate import evaluate_model, save_results")
