# Auto-generated by AI-Playground
# Generated: 2025-12-30T23:00:47.447829
# Experiment: Titanic Survival Prediction

import numpy as np
import pandas as pd
import json
from pathlib import Path
from typing import Dict, Any, Optional, Tuple

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix,
    classification_report,
    roc_curve,
    precision_recall_curve,
    average_precision_score
)

import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')


# ============================================================================
# CLASSIFICATION EVALUATION
# ============================================================================

def evaluate_model(
    model,
    X_test: np.ndarray,
    y_test: np.ndarray,
    y_pred: Optional[np.ndarray] = None,
    y_proba: Optional[np.ndarray] = None
) -> Dict[str, Any]:
    """
    Evaluate classification model performance.
    
    Args:
        model: Trained model
        X_test: Test features
        y_test: True labels
        y_pred: Predicted labels (optional, will be computed if not provided)
        y_proba: Predicted probabilities (optional, for AUC/ROC)
    
    Returns:
        Dictionary containing evaluation metrics
    """
    print("Evaluating classification model...")
    print(f"Test samples: {len(y_test)}")
    
    # Get predictions if not provided
    if y_pred is None:
        y_pred = model.predict(X_test)
    
    # Get probabilities if available and not provided
    if y_proba is None and hasattr(model, 'predict_proba'):
        y_proba = model.predict_proba(X_test)
    
    results = {
        'task_type': 'classification',
        'n_samples': len(y_test),
        'n_classes': len(np.unique(y_test)),
        'metrics': {}
    }
    
    # Accuracy
    accuracy = accuracy_score(y_test, y_pred)
    results['metrics']['accuracy'] = float(accuracy)
    print(f"Accuracy: {accuracy:.4f}")
    
    # Precision
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    results['metrics']['precision'] = float(precision)
    print(f"Precision: {precision:.4f}")
    
    # Recall
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    results['metrics']['recall'] = float(recall)
    print(f"Recall: {recall:.4f}")
    
    # F1 Score
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    results['metrics']['f1_score'] = float(f1)
    print(f"F1 Score: {f1:.4f}")
    
    # ROC AUC (if probabilities available)
    if y_proba is not None:
        try:
            if results['n_classes'] == 2:
                # Binary classification
                roc_auc = roc_auc_score(y_test, y_proba[:, 1])
            else:
                # Multi-class classification
                roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr', average='weighted')
            results['metrics']['roc_auc'] = float(roc_auc)
            print(f"ROC AUC: {roc_auc:.4f}")
        except Exception as e:
            print(f"Could not compute ROC AUC: {e}")
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    results['confusion_matrix'] = cm.tolist()
    print(f"\nConfusion Matrix:\n{cm}")
    
    # Classification Report
    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
    results['classification_report'] = report
    
    print(f"\nEvaluation complete!")
    return results


# ============================================================================
# VISUALIZATION FUNCTIONS
# ============================================================================

def plot_confusion_matrix(cm: np.ndarray, save_path: Optional[str] = None):
    """
    Plot confusion matrix.
    
    Args:
        cm: Confusion matrix
        save_path: Path to save plot (optional)
    """
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Confusion matrix saved to {save_path}")
    else:
        plt.show()
    
    plt.close()

def plot_roc_curve(y_test: np.ndarray, y_proba: np.ndarray, save_path: Optional[str] = None):
    """
    Plot ROC curve for binary classification.
    
    Args:
        y_test: True labels
        y_proba: Predicted probabilities
        save_path: Path to save plot (optional)
    """
    fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1])
    roc_auc = roc_auc_score(y_test, y_proba[:, 1])
    
    plt.figure(figsize=(10, 8))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.grid(alpha=0.3)
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ROC curve saved to {save_path}")
    else:
        plt.show()
    
    plt.close()


# ============================================================================
# SAVE RESULTS
# ============================================================================

def save_results(results: Dict[str, Any], filepath: str = 'titanic_evaluation_results.json'):
    """
    Save evaluation results to JSON file.
    
    Args:
        results: Evaluation results dictionary
        filepath: Path to save results
    """
    # Ensure directory exists
    Path(filepath).parent.mkdir(parents=True, exist_ok=True)
    
    # Save to JSON
    with open(filepath, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nResults saved to {filepath}")


# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == '__main__':
    """
    Example usage of evaluation functions.
    """
    print("=" * 80)
    print("Model Evaluation - Titanic Survival Prediction")
    print("=" * 80)
    
    # Example: Load your model and test data
    # import pickle
    # with open('model.pkl', 'rb') as f:
    #     model = pickle.load(f)
    # 
    # X_test = ...  # Load test features
    # y_test = ...  # Load test labels
    
    # Evaluate model
    # results = evaluate_model(model, X_test, y_test)
    
    # Save results
    # save_results(results, 'titanic_evaluation_results.json')
    
    # Plot confusion matrix
    # if 'confusion_matrix' in results:
    #     plot_confusion_matrix(np.array(results['confusion_matrix']), 'confusion_matrix.png')
    
    # Plot ROC curve (for binary classification)
    # if hasattr(model, 'predict_proba'):
    #     y_proba = model.predict_proba(X_test)
    #     plot_roc_curve(y_test, y_proba, 'roc_curve.png')
    
    print("\nEvaluation script ready!")
    print("Uncomment the example code above to run evaluation.")
